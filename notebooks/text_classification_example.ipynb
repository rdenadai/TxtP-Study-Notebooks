{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/TxtP-Study-Notebooks/blob/master/notebooks/text_classification_example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WGxO__uG47K5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Análise e Validação de Textos em Português\n"
      ]
    },
    {
      "metadata": {
        "id": "AzVNkORP5dvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Referências:\n",
        "\n",
        " - [NLTK](http://www.nltk.org/howto/portuguese_en.html)\n",
        " - [spaCy](https://spacy.io/usage/spacy-101)\n",
        " - [Utilizando processamento de linguagem natural para criar uma sumarização automática de textos](https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e)\n",
        " - [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
        " - [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
        " - [Unsupervised Emotion Detection from Text using Semantic and Syntactic Relations](http://www.cse.yorku.ca/~aan/research/paper/Emo_WI10.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "fJl2_xgi5M4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Instalação"
      ]
    },
    {
      "metadata": {
        "id": "6gSt861Txvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "outputId": "f976a88c-333c-4945-f4e2-ae0d690984d6"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt\n",
        "# !pip install feedparser"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 18.2MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz (38.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.7MB 31.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "  Running setup.py install for pt-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed pt-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PcT7xbKKo55-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "4d1f1205-065d-4256-dea1-7fd635e5f82d"
      },
      "cell_type": "code",
      "source": [
        "# Download Oplexicon\n",
        "!rm -rf wget-log*\n",
        "!rm -rf oplexicon_v3.0\n",
        "!wget -O oplexicon_v3.0.zip https://github.com/rdenadai/sentiment-analysis-2018-president-election/blob/master/dataset/oplexicon_v3.0.zip?raw=true\n",
        "!unzip oplexicon_v3.0.zip\n",
        "!ls -lh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "Archive:  oplexicon_v3.0.zip\n",
            "  inflating: oplexicon_v3.0/lexico_v3.0.txt  \n",
            "  inflating: oplexicon_v3.0/README   \n",
            "total 120K\n",
            "drwxr-xr-x 2 root root 4.0K Oct  5 17:55 oplexicon_v3.0\n",
            "-rw-r--r-- 1 root root 102K Oct  5 17:55 oplexicon_v3.0.zip\n",
            "drwxr-xr-x 2 root root 4.0K Sep 28 23:32 sample_data\n",
            "-rw-r--r-- 1 root root 1.6K Oct  5 17:55 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVxM4zTo5QSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HLlEa6uEyX11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "830b6b88-db89-466e-b337-42d7279c098a"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('floresta')\n",
        "nltk.download('mac_morpho')\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import concurrent.futures\n",
        "import codecs\n",
        "import re\n",
        "import pprint\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.linalg import svd\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import floresta as flt\n",
        "from nltk.corpus import machado as mch\n",
        "from nltk.corpus import mac_morpho as mcm\n",
        "\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Package floresta is already up-to-date!\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Package mac_morpho is already up-to-date!\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0CGK0ftshPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ]
    },
    {
      "metadata": {
        "id": "pOMbwFd3sd31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_oplexicon_data(filename):\n",
        "    spacy_conv = {\n",
        "        'adj': 'ADJ',\n",
        "        'n': 'NOUN',\n",
        "        'vb': 'VERB',\n",
        "        'det': 'DET',\n",
        "        'emot': 'EMOT',\n",
        "        'htag': 'HTAG'\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    with codecs.open(filename, 'r', 'UTF-8') as hf:\n",
        "        lines = hf.readlines()\n",
        "        for line in lines:\n",
        "            info = line.lower().split(',')\n",
        "            if len(info[0].split()) <= 1:\n",
        "                info[1] = [spacy_conv.get(tag) for tag in info[1].split()]\n",
        "                word, tags, sent = info[:3]\n",
        "                if 'HTAG' not in tags and 'EMOT' not in tags:\n",
        "                    word = word.replace('-se', '')\n",
        "                    stem = stemmer.stem(word)\n",
        "                    if stem in data:\n",
        "                        data[stem] += [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "                    else:\n",
        "                        data[stem] = [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwSQhPMk5Scp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ]
    },
    {
      "metadata": {
        "id": "mQzbtaNhy5Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "26a07c17-f9fd-4c30-d7fc-d6fd34a313bc"
      },
      "cell_type": "code",
      "source": [
        "frase = u\"Gostaria de saber mais informações sobre a Amazon. Uma excelente loja de produtos online!\".lower()\n",
        "doc = nlp(frase)\n",
        "pp.pprint([(w.text, w.pos_) for w in doc])\n",
        "\n",
        "# for dc in doc:\n",
        "#     if dc.pos_ == 'VERB':\n",
        "#         print(dc.lemma_)\n",
        "#     else:\n",
        "#         print(dc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   ('gostaria', 'VERB'),\n",
            "    ('de', 'ADP'),\n",
            "    ('saber', 'VERB'),\n",
            "    ('mais', 'DET'),\n",
            "    ('informações', 'NOUN'),\n",
            "    ('sobre', 'ADP'),\n",
            "    ('a', 'DET'),\n",
            "    ('amazon', 'NOUN'),\n",
            "    ('.', 'PUNCT'),\n",
            "    ('uma', 'DET'),\n",
            "    ('excelente', 'ADJ'),\n",
            "    ('loja', 'NOUN'),\n",
            "    ('de', 'ADP'),\n",
            "    ('produtos', 'NOUN'),\n",
            "    ('online', 'ADJ'),\n",
            "    ('!', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F--aytOy3gL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "33059fd3-4b6a-42bd-b98e-faaedbec8b78"
      },
      "cell_type": "code",
      "source": [
        "opx = load_oplexicon_data('oplexicon_v3.0/lexico_v3.0.txt')\n",
        "print('Oplexicon size: ', len(opx))\n",
        "print('Examples: ')\n",
        "\n",
        "view = opx.items()\n",
        "pp.pprint(list(view)[:7])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oplexicon size:  10687\n",
            "Examples: \n",
            "[   ('ab-rog', [{'sentiment': '-1', 'tags': ['VERB'], 'word': ['ab-rogar']}]),\n",
            "    ('ababad', [{'sentiment': '0', 'tags': ['VERB'], 'word': ['ababadar']}]),\n",
            "    (   'ababel',\n",
            "        [   {'sentiment': '-1', 'tags': ['VERB'], 'word': ['ababelar']},\n",
            "            {'sentiment': '1', 'tags': ['VERB'], 'word': ['ababelar']}]),\n",
            "    ('abaçan', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abaçanar']}]),\n",
            "    ('abacin', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abacinar']}]),\n",
            "    (   'abaf',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafados']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafante']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafantes']},\n",
            "            {'sentiment': '-1', 'tags': ['VERB'], 'word': ['abafar']}]),\n",
            "    (   'abaix',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixados']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']}])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AcajLBiFm3N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ALEGRIA = ['abundante', 'acalmar', 'aceitável', 'aclamar', 'aconchego', 'adesão', 'admirar', 'adorar', 'afável', 'afeição', 'afeto', 'afortunado', 'agradar', 'ajeitar', 'alívio', 'amabilidade', 'amado', 'amar', 'amável', 'amenizar', 'ameno', 'amigável', 'amistoso', ' amizade', ' amor', ' animação', ' ânimo', 'anseio', 'ânsia', 'ansioso', 'apaixonado', 'apaziguar', 'aplausos', 'apoiar', 'aprazer', 'apreciar', 'aprovação', 'aproveitar', 'ardor', 'armirar', 'arrumar', 'atração', 'atraente', 'atrair', 'avidamente', 'avidez', 'ávido', 'belo', 'bem-estar', 'beneficência', 'beneficiador', 'benefício', 'benéfico', 'benevocência', 'benignamente', 'benígno', 'bom', 'bondade', 'bondoso', 'bonito', 'brilhante', 'brincadeira', 'calma', 'calor', 'caridade', 'caridoso', 'carinho', 'cativar', 'charme', 'cheery', 'clamar', 'cofortar', 'coleguismo', 'comédia', 'cômico', 'comover', 'compaixão', 'companheirismo', 'compatibilidade', 'compatível', 'complacência', 'completar', 'compreensão', 'conclusão', 'concretização', 'condescendência', 'confiança', 'confortante', 'congratulação', 'conquistar', 'consentir', 'consideração', 'consolação', 'contentamento', 'coragem', 'cordial', 'considerar', 'consolo', 'contente', 'cuidadoso', 'cumplicidade', 'dedicação', 'deleitado', 'delicadamente', 'delicadeza', 'delicado', 'desejar', 'despreocupação', 'devoção', 'devoto', 'diversão', 'divertido', 'encantar', 'elogiado', 'emoção', 'emocionante', 'emotivo', 'empatia', 'empático', 'empolgação', 'enamorar', 'encantado', 'encorajado', 'enfeitar', 'engraçado', 'entendimento', 'entusiasmadamente', 'entusiástico', 'esperança', 'esplendor', 'estima', 'estimar', 'estimulante', 'euforia', 'eufórico', 'euforizante', 'exaltar', 'excelente', 'excitar', 'expansivo', 'extasiar', 'exuberante', 'exultar', 'fã', 'facilitar', 'familiaridade', 'fascinação', 'fascínio', 'favor', 'favorecer', 'favorito', 'felicidade', 'feliz', 'festa', 'festejar', 'festivo', 'fidelidade', 'fiel', 'filantropia', 'filantrópico', 'fraterno', 'ganhar', 'generosidade', 'generoso', 'gentil', 'glória', 'glorificar', 'gostar', 'gostoso', 'gozar', 'gratificante', 'grato', 'hilariante', 'honra', 'humor', 'impressionar', 'incentivar', 'incentivo', 'inclinação', 'incrível', 'inspirar', 'interessar', 'interesse', 'irmandade', 'jovial', 'jubilante', 'júbilo', 'lealdade', 'legítimo', 'leveza', 'louvar', 'louvável', 'louvavelmente', 'lucrativo', 'lucro', 'maravilhoso', 'melhor', 'obter', 'obteve', 'ode', 'orgulho', 'paixão', 'parabenizar', 'paz', 'piedoso', 'positivo', 'prazenteiro', 'prazer', 'predileção', 'preencher', 'preferência', 'preferido', 'promissor', 'prosperidade', 'proteção', 'proteger', 'revigorar', 'simpático', 'vantajoso', 'protetor', 'risada', 'sobrevivência', 'vencedor', 'proveito', 'risonho', 'sobreviver', 'veneração', 'provilégio', 'romântico', 'sorte', 'ventura', 'querer', 'romantismo', 'sortudo', 'vida', 'radiante', 'saciar', 'sucesso', 'vigor', 'realizar', 'saciável', 'surpreender', 'virtude', 'recomendável', 'satisfação', 'tenro', 'virtuoso', 'reconhecer', 'satisfatoriamente', 'ternura', 'vitória', 'recompensa', 'satisfatório', 'torcer', 'vitorioso', 'recrear', 'satisfazer', 'tranquilo', 'viver', 'recreativo', 'satisfeito', 'tranquilo', 'vivo', 'recreação', 'sedução', 'triunfo', 'zelo', 'regozijar', 'seduzir', 'triunfal', 'zeloso', 'respeitar', 'sereno', 'triunfante', 'ressuscitar', 'simpaticamente', 'vantagem',]\n",
        "DESGOSTO = ['abominável', 'adoentado', 'amargamente', 'antipatia', 'antipático', 'asco', 'asqueroso', 'aversão', 'chatear', 'chateação', 'desagrado', 'desagradável', 'desprezível', 'detestável', 'doente', 'doença', 'enfermidade', 'enjoativo', 'enjoo', 'enjôo', 'feio', 'fétido', 'golfar', 'grave', 'gravidade', 'grosseiro', 'grosso', 'horrível', 'ignóbil', 'ilegal', 'incomodar', 'incômdo', 'indecente', 'indisposição', 'indisposto', 'inescrupuloso', 'maldade', 'maldoso', 'malvado', 'mau', 'nauseabundo', 'nauseante', 'nausear', 'nauseoso', 'nojento', 'nojo', 'náusea', 'obsceno', 'obstruir', 'obstrução', 'ofensivo', 'patético', 'perigoso', 'repelente', 'repelir', 'repugnante', 'repulsa', 'repulsivo', 'repulsão', 'rude', 'sujeira', 'sujo', 'terrivelmente', 'terrível', 'torpe', 'travesso', 'travessura', 'ultrajante', 'vil', 'vomitar', 'vômito',]\n",
        "MEDO = ['abominável', 'afugentar', 'alarmar', 'alerta', 'ameaça', 'amedrontar', 'angustia', 'angústia', 'angustiadamente', 'ansiedade', 'ansioso', 'apavorar', 'apreender', 'apreensão', 'apreensivo', 'arrepio', 'assombrado', 'assombro', 'assustado', 'assustadoramente', 'atemorizar', 'aterrorizante', 'brutal', 'calafrio', 'chocado', 'chocante', 'consternado', 'covarde', 'cruel', 'crueldade', 'cruelmente', 'cuidado', 'cuidadosamente', 'cuidadoso', 'defender', 'defensor', 'defesa', 'derrotar', 'desconfiado', 'desconfiança', 'desencorajar', 'desespero', 'deter', 'envergonhado', 'escandalizado', 'escuridão', 'espantoso', 'estremecedor', 'estremecer', 'expulsar', 'feio', 'friamente', 'fugir', 'hesitar', 'horrendo', 'horripilante', 'horrível', 'horrivelmente', 'horror', 'horrorizar', 'impaciência', 'impaciente', 'impiedade', 'impiedoso', 'indecisão', 'inquieto', 'insegurança', 'inseguro', 'intimidar', 'medonho', 'medroso', 'monstruosamente', 'mortalha', 'nervoso', 'pânico', 'pavor', 'premonição', 'preocupar', 'presságio', 'pressentimento', 'recear', 'receativamente', 'receio', 'receoso', 'ruim', 'suspeita', 'suspense', 'susto', 'temer', 'tenso', 'terror', 'tremor', 'temeroso', 'terrificar', 'timidamente', 'vigiar', 'temor', 'terrível', 'timidez', 'vigilante', 'tensão', 'terrivelmente', 'tímido',]\n",
        "RAIVA = ['abominação', 'aborrecer', 'adredido', 'agredir', 'agressão', 'agressivo', 'amaldiçoado', 'amargor', 'amargura', 'amolar', 'angústia', 'animosidade', 'antipatia', 'antipático', 'asco', 'assassinar', 'assassinato', 'assediar', 'assédio', 'atormentar', 'avarento', 'avareza', 'aversão', 'beligerante', 'bravejar', 'chateação', 'chato', 'cobiçoso', 'cólera', 'colérico', 'complicar', 'contraiedade', 'contrariar', 'corrupção', 'corrupto', 'cruxificar', 'demoníaco', 'demônio', 'descaso', 'descontente', 'descontrole', 'desenganar', 'desgostar', 'desgraça', 'desprazer', 'desprezar', 'destruição', 'destruir', 'detestar', 'diabo', 'diabólico', 'doido', 'encolerizar', 'energicamente', 'enfurecido', 'enfuriante', 'enlouquecer', 'enraivecer', 'escandalizar', 'escândalo', 'escoriar', 'exasperar', 'execração', 'ferir', 'frustração', 'frustrar', 'fúria', 'furioso', 'furor', 'ganância', 'ganancioso', 'guerra', 'guerreador', 'guerrilha', 'hostil', 'humilhar', 'implicância', 'implicar', 'importunar', 'incomodar', 'incômodo', 'indignar', 'infernizar', 'inimigo', 'inimizade', 'injúria', 'injuriado', 'injustiça', 'insulto', 'malícia', 'odiável', 'repulsivo', 'inveja', 'malicioso', 'ódio', 'resmungar', 'ira', 'malignidade', 'odioso', 'ressentido', 'irado', 'malígno', 'ofendido', 'revolta', 'irascibilidade', 'maltratar', 'ofensa', 'ridículo', 'irascível', 'maluco', 'opressão', 'tempestuoso', 'irritar', 'malvadeza', 'opressivo', 'tirano', 'louco', 'malvado', 'oprimir', 'tormento', 'loucura', 'matar', 'perseguição', 'torturar', 'magoar', 'mesquinho', 'perseguir', 'ultrage', 'mal', 'misantropia', 'perturbar', 'ultrajar', 'maldade', 'misantrópico', 'perverso', 'vexatório', 'maldição', 'molestar', 'provocar', 'vigoroso', 'maldito', 'moléstia', 'rabugento', 'vingança', 'maldizer', 'mortal', 'raivoso', 'vingar', 'maldoso', 'morte', 'rancor', 'vingativo', 'maleficência', 'mortífero', 'reclamar', 'violência', 'maléfico', 'mortificar', 'repressão', 'violento', 'malevolência', 'nervoso', 'reprimir', 'zangar', 'malévolo', 'odiar', 'repulsa',]\n",
        "SURPRESA = ['admirar', 'afeição', 'apavorante', 'assombro', 'chocado', 'chocante', 'desconcertar', 'deslumbrar', 'embasbacar', 'emudecer', 'encantamento', 'enorme', 'espanto', 'estupefante', 'estupefato', 'estupefazer', 'expectativa', 'fantasticamente', 'fantástico', 'horripilante', 'imaginário', 'imenso', 'impressionado', 'incrível', 'maravilha', 'milagre', 'mistério', 'misterioso', 'ótimo', 'pasmo', 'perplexo', 'prodígio', 'sensacional', 'surpreendente', 'surpreender', 'suspense', 'susto', 'temor', 'tremendo',]\n",
        "TRISTEZA = ['abandonar', 'abatido', 'abominável', 'aborrecer', 'abortar', 'afligir', 'aflito', 'aflição', 'agoniar', 'amargo', 'amargor', 'amargura', 'ansiedade', 'arrepender', 'arrependidamente', 'atrito', 'azar', 'cabisbaixo', 'choro', 'choroso', 'chorão', 'coitado', 'compassivo', 'compunção', 'contristador', 'contrito', 'contrição', 'culpa', 'defeituoso', 'degradante', 'deplorável', 'deposição', 'depravado', 'depressivo', 'depressão', 'deprimente', 'deprimir', 'derrota', 'derrubar', 'desalentar', 'desamparo', 'desanimar', 'desapontar', 'desconsolo', 'descontente', 'desculpas', 'desencorajar', 'desespero', 'desgaste', 'desgosto', 'desgraça', 'desistir', 'desistência', 'deslocado', 'desmoralizar', 'desolar', 'desonra', 'despojado', 'desprazer', 'desprezo', 'desumano', 'desânimo', 'discriminar', 'disforia', 'disfórico', 'dissuadir', 'doloroso', 'dor', 'dó', 'enfadado', 'enlutar', 'entediado', 'entristecedor', 'entristecer', 'envergonhar', 'errante', 'erro', 'errôneo', 'escurecer', 'escuridão', 'escuro', 'esquecido', 'estragado', 'execrável', 'extirpar', 'falsidade', 'falso', 'falta', 'fraco', 'fraqueza', 'fricção', 'frieza', 'frio', 'funesto', 'fúnebre', 'grave', 'horror', 'humilhar', 'inconsolável', 'indefeso', 'infelicidade', 'infeliz', 'infortúnio', 'isolar', 'lacrimejante', 'lacrimoso', 'lamentar', 'lastimoso', 'luto', 'lutoso', 'lágrima', 'lástima', 'lúgubre', 'magoar', 'martirizar', 'martírio', 'mau', 'melancolia', 'melancólico', 'menosprezar', 'miseravelmente', 'misterioso', 'mistério', 'miséria', 'morre', 'morte', 'mortificante', 'mágoa', 'negligentemente', 'nocivo', 'obscuro', 'opressivo', 'opressão', 'oprimir', 'pena', 'penalizar', 'penitente', 'penoso', 'penumbra', 'perder', 'perturbado', 'perverso', 'pervertar', 'pesaroso', 'pessimamente', 'piedade', 'pobre', 'porcamente', 'prejudicado', 'prejudicial', 'prejuízo', 'pressionar', 'pressão', 'quebrar', 'queda', 'queixoso', 'rechaçar', 'remorso', 'repressivo', 'repressão', 'reprimir', 'ruim', 'secreto', 'servil', 'sobrecarga', 'sobrecarregado', 'sofrer', 'sofrimento', 'solidão', 'sombrio', 'soturno', 'sujo', 'suplicar', 'suplício', 'só', 'timidez', 'torturar', 'trevas', 'triste', 'tristemente', 'tédio', 'tímido', 'vazio',]\n",
        "\n",
        "emotion_words = {\n",
        "    'ALEGRIA': ALEGRIA,\n",
        "    'DESGOSTO': DESGOSTO,\n",
        "    'MEDO': MEDO,\n",
        "    'RAIVA': RAIVA,\n",
        "    'SURPRESA': SURPRESA,\n",
        "    'TRISTEZA': TRISTEZA,\n",
        "}\n",
        "for key, values in words.items():\n",
        "    for i, word in enumerate(values):\n",
        "        emotion_words[key][i] = ''.join([p.lemma_ for p in nlp(word.lower())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zf-PxyeIwi0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d2118947-5183-486e-8261-a1e18cf0966d"
      },
      "cell_type": "code",
      "source": [
        "stpwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "# stpwords = set(list(punctuation))\n",
        "\n",
        "def tokenize_frases(frase):\n",
        "    return word_tokenize(frase.lower())\n",
        "\n",
        "def rm_stop_words_tokenized(frase):\n",
        "    frase = nlp(frase.lower())\n",
        "    clean_frase = []\n",
        "    for palavra in frase:\n",
        "        if palavra.pos_ != 'PUNCT':\n",
        "            palavra = palavra.lemma_\n",
        "            if palavra not in stpwords and not palavra.isdigit():\n",
        "                clean_frase.append(palavra)\n",
        "    return ' '.join(filter(None, clean_frase))\n",
        "\n",
        "def generate_corpus(frases, tokenize=False):\n",
        "    print('Iniciando processamento...')\n",
        "    tokenized_frases = frases\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "        if tokenize:\n",
        "            print('Executando processo de tokenização das frases...')\n",
        "            tokenized_frases = procs.map(tokenize_frases, frases, chunksize=25)\n",
        "        print('Executando processo de remoção das stopwords...')\n",
        "        tokenized_frases = procs.map(rm_stop_words_tokenized, tokenized_frases, chunksize=25)\n",
        "    print('Filtro e finalização...')\n",
        "    return tokenized_frases\n",
        "\n",
        "\n",
        "frases = [\n",
        "    'Bom dia SENADOR, agora está claro porque o pedágio não baixava,o judiciário não se manifestava quando era provocado e as CPIs só serviram prá corrupção,deu no que deu 🙄',\n",
        "    'Não basta apenas retirar o candidato preferencial da maioria dos eleitores brasileiros. Tem que impedir também que esses mesmos eleitores possam comparecer às urnas. Que democracia é essa, minha gente? Poder judiciário comprometido até os cabelos com o golpe de destrói o país.',\n",
        "    'Deus abençoe o dia de todos você, tenham um bom trabalho e bom estudo a todos. E pra aqueles que não trabalha e nem estuda, boa curtição em sua cama 🙂',\n",
        "    'Aprenda a ter amor próprio que nem essa banana q fez uma tatuagem dela mesma.',\n",
        "    'Estou muito feliz hoje',\n",
        "    'Dias chuvosos me deixam triste',\n",
        "    'Hoje o dia esta excelente',\n",
        "]\n",
        "\n",
        "N = 10000\n",
        "# frases = flt.sents()[:N] + mch.sents()[:N] + mcm.sents()[:N]\n",
        "\n",
        "frases = list(generate_corpus(frases, tokenize=False))\n",
        "print(frases)\n",
        "\n",
        "ldocs = [f'D{i}' for i in range(len(frases))]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando processamento...\n",
            "Executando processo de remoção das stopwords...\n",
            "Filtro e finalização...\n",
            "['bom dia senador agora estar claro porque pedágio baixar judiciário manifestar ser provocar cpis servir prá corrupção dar dar 🙄', 'basto apenas retirar candidatar preferencial maioria eleitor brasileiro ter impedir eleitor poder comparecer s urna democracia ser gente poder judiciário comprometer cabelo golpe destruir país', 'deus abençoar dia todo ter bom trabalhar bom estudar todo pra trabalhar estudar bom curtição suar cama 🙂', 'aprender ter amor próprio banana q fazer umar tatuagem d', 'estar feliz hoje', 'dia chuvoso deixar triste', 'hoje dia excelente']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7CXkPtuVzio8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "15ae2736-b482-4d0d-d7be-9d31eecd4171"
      },
      "cell_type": "code",
      "source": [
        "print('Tf-Idf:')\n",
        "vectorizer = TfidfVectorizer(max_df=1, sublinear_tf=True, use_idf=True, ngram_range=(1, 1))\n",
        "X_tfidf = vectorizer.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_tfidf.get_shape()[1])\n",
        "weights_df = pd.DataFrame(np.round(X_tfidf.toarray().T, 3), index=vectorizer.get_feature_names(), columns=ldocs)\n",
        "display(weights_df.head(15))"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-Idf:\n",
            "   Actual number of tfidf features: 53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abençoar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brasileiro</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cabelo</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cama</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>candidatar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chuvoso</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.577</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claro</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comparecer</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               D0    D1     D2     D3   D4     D5   D6\n",
              "abençoar    0.000  0.00  0.262  0.000  0.0  0.000  0.0\n",
              "agora       0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "amor        0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "apenas      0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "aprender    0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "baixar      0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "banana      0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "basto       0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "brasileiro  0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "cabelo      0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "cama        0.000  0.00  0.262  0.000  0.0  0.000  0.0\n",
              "candidatar  0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "chuvoso     0.000  0.00  0.000  0.000  0.0  0.577  0.0\n",
              "claro       0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "comparecer  0.000  0.21  0.000  0.000  0.0  0.000  0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GbRzyK7Zn4G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "f7386896-431b-447c-e1e9-a0f1973de73e"
      },
      "cell_type": "code",
      "source": [
        "print('Count:')\n",
        "vectorizer = CountVectorizer()\n",
        "X_count = vectorizer.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_count.get_shape()[1])\n",
        "weights_df = pd.DataFrame(X_count.toarray().T, index=vectorizer.get_feature_names(), columns=ldocs)\n",
        "display(weights_df.head(15))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            "   Actual number of tfidf features: 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abençoar</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bom</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brasileiro</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cabelo</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cama</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>candidatar</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chuvoso</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claro</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            D0  D1  D2  D3  D4  D5  D6\n",
              "abençoar     0   0   1   0   0   0   0\n",
              "agora        1   0   0   0   0   0   0\n",
              "amor         0   0   0   1   0   0   0\n",
              "apenas       0   1   0   0   0   0   0\n",
              "aprender     0   0   0   1   0   0   0\n",
              "baixar       1   0   0   0   0   0   0\n",
              "banana       0   0   0   1   0   0   0\n",
              "basto        0   1   0   0   0   0   0\n",
              "bom          1   0   3   0   0   0   0\n",
              "brasileiro   0   1   0   0   0   0   0\n",
              "cabelo       0   1   0   0   0   0   0\n",
              "cama         0   0   1   0   0   0   0\n",
              "candidatar   0   1   0   0   0   0   0\n",
              "chuvoso      0   0   0   0   0   1   0\n",
              "claro        1   0   0   0   0   0   0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uvfJwFa8atEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "36487e22-fb07-4ec9-862b-81839c597485"
      },
      "cell_type": "code",
      "source": [
        "print('SVD: ')\n",
        "AC = copy.deepcopy(X_count.toarray().T)\n",
        "u, s, v = np.linalg.svd(AC, full_matrices=False)\n",
        "print('Original and SVD equals: ', np.allclose(AC, np.dot(u, np.dot(np.diag(s), v))))\n",
        "\n",
        "# print(AC)\n",
        "# print(u.astype(np.float16))\n",
        "# print('-' * 20)\n",
        "# print(np.diag(s.astype(np.float16)))\n",
        "# print('-' * 20)\n",
        "# print(v.astype(np.float16))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVD: \n",
            "Original and SVD equals:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tCgjE9sRbKls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hmm-lda\n",
        "# https://ieeexplore.ieee.org/document/7363382\n",
        "# https://link.springer.com/chapter/10.1007/978-3-642-21802-6_57\n",
        "# http://www.ppgia.pucpr.br/~paraiso/Projects/Emocoes/Emocoes.html\n",
        "\n",
        "emotion_weigths = {}\n",
        "for k, items in enumerate(emotion_words.items()):\n",
        "    key, values = items\n",
        "    emotion_weigths[key] = []\n",
        "    for i, word in enumerate(values):\n",
        "        idx_val = 0\n",
        "        w = 0\n",
        "        try:\n",
        "            index = weights_df.index.get_loc(word)\n",
        "            idx_val = u[index]\n",
        "            w = weights_df.iloc[index].values\n",
        "        except:\n",
        "            w = np.zeros((len(ldocs, )))\n",
        "        emotion_weigths[key].append(idx_val * w)\n",
        "# pp.pprint(emotion_weigths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zctaT8xVmiJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a3db493b-4ed4-4e2b-e1d2-b6a7a09c91f5"
      },
      "cell_type": "code",
      "source": [
        "dtframe = {d: np.zeros(len(ldocs)) for d in emotion_weigths}\n",
        "for k, item in enumerate(emotion_weigths.items()):\n",
        "    sent = np.array(item[1])\n",
        "    for i, m in enumerate(cosine_distances(v.T, sent)):\n",
        "        print(m)\n",
        "        dtframe[item[0]][i] = np.sum(m)\n",
        "\n",
        "# pp.pprint(dtframe)\n",
        "df = pd.DataFrame(list(dtframe.values()), index=dtframe.keys(), columns=ldocs)\n",
        "display(df.head(15))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4e0c349dca09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_weigths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_weigths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'emotion_weigths' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "x7TwnB5okQkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyW_HKxkwURE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9277f0ea-40b5-45a2-8f93-f6656898a770"
      },
      "cell_type": "code",
      "source": [
        "print(\"LSA using TruncatedSVD:\")\n",
        "\n",
        "# Project the tfidf vectors onto the first N principal components.\n",
        "# Though this is significantly fewer features than the original tfidf vector,\n",
        "# they are stronger features, and the accuracy is higher.\n",
        "svd = TruncatedSVD(50)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_lsa = lsa.fit_transform(X_count)\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(\"   Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
        "\n",
        "print(svd.explained_variance_.shape)\n",
        "print(svd.singular_values_.shape) # S\n",
        "print(svd.components_.shape) # VT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using TruncatedSVD:\n",
            "   Explained variance of the SVD step: 100%\n",
            "(4,)\n",
            "(4,)\n",
            "(4, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHnVMukg0oWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "008bfebd-37e4-4bc8-9530-f96c9d250313"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using numpy:')\n",
        "u, s, v = np.linalg.svd(X_tfidf.toarray(), full_matrices=False)\n",
        "print(u)\n",
        "print(s.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using numpy:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]]\n",
            "(4,)\n",
            "(4, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_KCInH7aaUQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9bc73216-ab28-4f39-c1ed-2a39a7af4076"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using scikit-learn randomized_svd:')\n",
        "U, Sigma, VT = randomized_svd(X_count, \n",
        "                              n_components=50,\n",
        "                              n_iter=5,\n",
        "                              random_state=None)\n",
        "print(U.shape)\n",
        "print(Sigma.shape)\n",
        "print(VT.shape)\n",
        "\n",
        "print(U)\n",
        "# print(VT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using scikit-learn randomized_svd:\n",
            "(4, 4)\n",
            "(4,)\n",
            "(4, 61)\n",
            "[[ 8.23887410e-02  4.64153487e-01  8.81914756e-01  0.00000000e+00]\n",
            " [ 9.96228666e-01 -6.25206048e-02 -6.01632624e-02 -3.70560802e-16]\n",
            " [ 2.72128558e-02  8.83545536e-01 -4.67554003e-01  2.07365235e-16]\n",
            " [ 3.63520293e-16 -2.06384313e-16  7.46602991e-17  1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xy-B3w6obyOj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b452f4f1-74d8-4410-937d-aeeff78b48f7"
      },
      "cell_type": "code",
      "source": [
        "# define a matrix\n",
        "# A = array([[1, 2], [3, 4], [5, 6]])\n",
        "A = np.array([\n",
        "    [1, 1, 1, 0, 0],\n",
        "    [3, 3, 3, 0, 0],\n",
        "    [4, 4, 4, 0, 0],\n",
        "    [5, 5, 5, 0, 0],\n",
        "    [0, 2, 0, 4, 4],\n",
        "    [0, 0, 0, 5, 5],\n",
        "    [0, 1, 0, 2, 2],\n",
        "])\n",
        "print(A.shape)\n",
        "\n",
        "\n",
        "u, s, v = np.linalg.svd(copy.deepcopy(A), full_matrices=False)\n",
        "\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, 5)\n",
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307 ]\n",
            " [-0.1528   0.5913   0.654    0.       0.2    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4    ]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0LFVgEqGyLRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "99d47291-3c92-4804-a74f-3fd7c0174045"
      },
      "cell_type": "code",
      "source": [
        "u, s, v = randomized_svd(copy.deepcopy(A), \n",
        "                          power_iteration_normalizer='auto',\n",
        "                          flip_sign=True,\n",
        "                          n_components=100,\n",
        "                          n_iter=1,\n",
        "                          random_state=None)\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.3757e-01 -2.3605e-02  1.0811e-02  9.3652e-01  2.8711e-01]\n",
            " [ 4.1284e-01 -7.0862e-02  3.2440e-02 -3.1152e-01  8.4912e-01]\n",
            " [ 5.5029e-01 -9.4421e-02  4.3243e-02  1.2622e-01 -2.9834e-01]\n",
            " [ 6.8799e-01 -1.1804e-01  5.4047e-02 -1.0132e-01 -3.2812e-01]\n",
            " [ 1.5283e-01  5.9131e-01 -6.5381e-01 -1.5199e-04 -6.2406e-05]\n",
            " [ 7.2205e-02  7.3145e-01  6.7822e-01  0.0000e+00  0.0000e+00]\n",
            " [ 7.6416e-02  2.9565e-01 -3.2690e-01  3.0398e-04  1.2481e-04]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[ 0.5625   0.593    0.5625   0.09015  0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [ 0.4097  -0.8047   0.4097   0.09125  0.09125]\n",
            " [ 0.707    0.      -0.707   -0.      -0.     ]\n",
            " [-0.      -0.       0.       0.707   -0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F4A5h4-8zNqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f91544f2-d7be-432e-dc8b-809779aea0dd"
      },
      "cell_type": "code",
      "source": [
        "u, s, v = svd(copy.deepcopy(A))\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757  -0.7     -0.1879 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756   -0.258    0.378  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846  -0.344   -0.0923 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307   0.57    -0.11536]\n",
            " [-0.1528   0.5913   0.654    0.       0.2      0.      -0.4    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.       0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4      0.       0.8    ]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmepFJucztDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "fe87e36b-0a46-4753-f535-97309438db11"
      },
      "cell_type": "code",
      "source": [
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.astype(np.float16))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757  -0.7     -0.1879 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756   -0.258    0.378  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846  -0.344   -0.0923 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307   0.57    -0.11536]\n",
            " [-0.1528   0.5913   0.654    0.       0.2      0.      -0.4    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.       0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4      0.       0.8    ]]\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RZoerdmc3Bbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "b02defdb-2313-45ec-d392-fa827e97a3b6"
      },
      "cell_type": "code",
      "source": [
        "A = [\n",
        "    [1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6],\n",
        "    [7, 8]\n",
        "]\n",
        "\n",
        "U, s, VT = svd(A)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.T.astype(np.float16))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1525  -0.8228  -0.3945  -0.38   ]\n",
            " [-0.3499  -0.4214   0.2428   0.801  ]\n",
            " [-0.5474  -0.0201   0.6978  -0.4614 ]\n",
            " [-0.7446   0.381   -0.5464   0.04074]]\n",
            "[[14.266  0.   ]\n",
            " [ 0.     0.627]]\n",
            "[[-0.6416  0.767 ]\n",
            " [-0.767  -0.6416]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uK2Thy4WLSeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "983b2701-163d-4d3d-afa2-a2982748d01a"
      },
      "cell_type": "code",
      "source": [
        "A = [\n",
        "    [2, 4],\n",
        "    [1, 3],\n",
        "    [0, 0],\n",
        "    [0, 0]\n",
        "    # [0, -1],\n",
        "    # [-2, 1],\n",
        "    # [1, 0]\n",
        "]\n",
        "\n",
        "U, s, VT = svd(A, full_matrices=False)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.T.astype(np.float16))\n",
        "\n",
        "print(np.dot(U, U.T).astype(np.float16))\n",
        "print(np.dot(VT, VT.T).astype(np.float16))\n",
        "\n",
        "\n",
        "print(np.allclose(A, np.dot(U, np.dot(np.diag(s), VT))))\n",
        "\n",
        "print(np.dot(U, np.dot(np.diag(s), VT)).astype(np.float16))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.8174 -0.576 ]\n",
            " [-0.576   0.8174]\n",
            " [ 0.      0.    ]\n",
            " [ 0.      0.    ]]\n",
            "[[5.465 0.   ]\n",
            " [0.    0.366]]\n",
            "[[-0.4045 -0.9146]\n",
            " [-0.9146  0.4045]]\n",
            "[[ 1. -0.  0.  0.]\n",
            " [-0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.]]\n",
            "[[ 1. -0.]\n",
            " [-0.  1.]]\n",
            "True\n",
            "[[2. 4.]\n",
            " [1. 3.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GT2PeZ_bQH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}