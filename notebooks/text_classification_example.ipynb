{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/TxtP-Study-Notebooks/blob/master/notebooks/text_classification_example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WGxO__uG47K5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## AnÃ¡lise e ValidaÃ§Ã£o de Textos em PortuguÃªs\n"
      ]
    },
    {
      "metadata": {
        "id": "AzVNkORP5dvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ReferÃªncias:\n",
        "\n",
        " - [NLTK](http://www.nltk.org/howto/portuguese_en.html)\n",
        " - [spaCy](https://spacy.io/usage/spacy-101)\n",
        " - [Utilizando processamento de linguagem natural para criar uma sumarizaÃ§Ã£o automÃ¡tica de textos](https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e)\n",
        " - [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
        " - [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
        " - [Unsupervised Emotion Detection from Text using Semantic and Syntactic Relations](http://www.cse.yorku.ca/~aan/research/paper/Emo_WI10.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "fJl2_xgi5M4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### InstalaÃ§Ã£o"
      ]
    },
    {
      "metadata": {
        "id": "6gSt861Txvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "40379a89-2d56-4e4c-8db3-47d446056edc"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt\n",
        "# !pip install feedparser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.4MB 58.7MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz (38.7MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.7MB 54.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "  Running setup.py install for pt-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed pt-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PcT7xbKKo55-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4d8c334f-2c05-468d-9062-76e8346a08b4"
      },
      "cell_type": "code",
      "source": [
        "# Download Oplexicon\n",
        "!rm -rf wget-log*\n",
        "!rm -rf oplexicon_v3.0\n",
        "!wget -O oplexicon_v3.0.zip https://github.com/rdenadai/sentiment-analysis-2018-president-election/blob/master/dataset/oplexicon_v3.0.zip?raw=true\n",
        "!unzip oplexicon_v3.0.zip\n",
        "!ls -lh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to â€˜wget-logâ€™.\n",
            "Archive:  oplexicon_v3.0.zip\n",
            "  inflating: oplexicon_v3.0/lexico_v3.0.txt  \n",
            "  inflating: oplexicon_v3.0/README   \n",
            "total 120K\n",
            "drwxr-xr-x 2 root root 4.0K Oct  3 00:42 oplexicon_v3.0\n",
            "-rw-r--r-- 1 root root 102K Oct  3 00:42 oplexicon_v3.0.zip\n",
            "drwxr-xr-x 2 root root 4.0K Sep 28 23:32 sample_data\n",
            "-rw-r--r-- 1 root root 1.6K Oct  3 00:42 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVxM4zTo5QSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HLlEa6uEyX11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2e2e26fd-7158-482b-86b2-27280b0089cc"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('floresta')\n",
        "nltk.download('mac_morpho')\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import concurrent.futures\n",
        "import codecs\n",
        "import re\n",
        "import pprint\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import floresta as flt\n",
        "from nltk.corpus import machado as mch\n",
        "from nltk.corpus import mac_morpho as mcm\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Package floresta is already up-to-date!\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Package mac_morpho is already up-to-date!\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0CGK0ftshPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ]
    },
    {
      "metadata": {
        "id": "pOMbwFd3sd31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_oplexicon_data(filename):\n",
        "    spacy_conv = {\n",
        "        'adj': 'ADJ',\n",
        "        'n': 'NOUN',\n",
        "        'vb': 'VERB',\n",
        "        'det': 'DET',\n",
        "        'emot': 'EMOT',\n",
        "        'htag': 'HTAG'\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    with codecs.open(filename, 'r', 'UTF-8') as hf:\n",
        "        lines = hf.readlines()\n",
        "        for line in lines:\n",
        "            info = line.lower().split(',')\n",
        "            if len(info[0].split()) <= 1:\n",
        "                info[1] = [spacy_conv.get(tag) for tag in info[1].split()]\n",
        "                word, tags, sent = info[:3]\n",
        "                if 'HTAG' not in tags and 'EMOT' not in tags:\n",
        "                    word = word.replace('-se', '')\n",
        "                    stem = stemmer.stem(word)\n",
        "                    if stem in data:\n",
        "                        data[stem] += [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "                    else:\n",
        "                        data[stem] = [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwSQhPMk5Scp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ]
    },
    {
      "metadata": {
        "id": "mQzbtaNhy5Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ab1cb495-1463-4839-c2cb-ace93f55359b"
      },
      "cell_type": "code",
      "source": [
        "frase = u\"Gostaria de saber mais informaÃ§Ãµes sobre a Amazon. Uma excelente loja de produtos online!\".lower()\n",
        "doc = nlp(frase)\n",
        "pp.pprint([(w.text, w.pos_) for w in doc])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   ('gostaria', 'VERB'),\n",
            "    ('de', 'ADP'),\n",
            "    ('saber', 'VERB'),\n",
            "    ('mais', 'DET'),\n",
            "    ('informaÃ§Ãµes', 'NOUN'),\n",
            "    ('sobre', 'ADP'),\n",
            "    ('a', 'DET'),\n",
            "    ('amazon', 'NOUN'),\n",
            "    ('.', 'PUNCT'),\n",
            "    ('uma', 'DET'),\n",
            "    ('excelente', 'ADJ'),\n",
            "    ('loja', 'NOUN'),\n",
            "    ('de', 'ADP'),\n",
            "    ('produtos', 'NOUN'),\n",
            "    ('online', 'ADJ'),\n",
            "    ('!', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F--aytOy3gL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "84fc7ec1-7f3f-465a-c97c-a04be4409598"
      },
      "cell_type": "code",
      "source": [
        "opx = load_oplexicon_data('oplexicon_v3.0/lexico_v3.0.txt')\n",
        "print('Oplexicon size: ', len(opx))\n",
        "print('Examples: ')\n",
        "\n",
        "view = opx.items()\n",
        "pp.pprint(list(view)[:7])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oplexicon size:  10687\n",
            "Examples: \n",
            "[   ('ab-rog', [{'sentiment': '-1', 'tags': ['VERB'], 'word': ['ab-rogar']}]),\n",
            "    ('ababad', [{'sentiment': '0', 'tags': ['VERB'], 'word': ['ababadar']}]),\n",
            "    (   'ababel',\n",
            "        [   {'sentiment': '-1', 'tags': ['VERB'], 'word': ['ababelar']},\n",
            "            {'sentiment': '1', 'tags': ['VERB'], 'word': ['ababelar']}]),\n",
            "    ('abaÃ§an', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abaÃ§anar']}]),\n",
            "    ('abacin', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abacinar']}]),\n",
            "    (   'abaf',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafados']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafante']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafantes']},\n",
            "            {'sentiment': '-1', 'tags': ['VERB'], 'word': ['abafar']}]),\n",
            "    (   'abaix',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixados']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']}])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zf-PxyeIwi0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e8d4aaa9-7393-4399-ad0c-20f0efd27e1d"
      },
      "cell_type": "code",
      "source": [
        "stpwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "# stpwords = set(list(punctuation))\n",
        "\n",
        "def tokenize_frases(frase):\n",
        "    return word_tokenize(frase.lower())\n",
        "\n",
        "def rm_stop_words_tokenized(frase):\n",
        "    clean_frase = []\n",
        "    for palavra in frase:\n",
        "        if palavra not in stpwords and not palavra.isdigit():\n",
        "            clean_frase.append(re.sub(r'[\\\"\\'!@#$%&*\\(\\)-_=+{}\\[\\]:;>.<,|\\\\`Â´]', '', palavra.lower()))\n",
        "        else:\n",
        "            clean_frase.append(None)\n",
        "    return ' '.join(filter(None, clean_frase))\n",
        "\n",
        "def generate_corpus(frases, tokenize=False):\n",
        "    print('Iniciando processamento...')\n",
        "    tokenized_frases = frases\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "        if tokenize:\n",
        "            print('Executando processo de tokenizaÃ§Ã£o das frases...')\n",
        "            tokenized_frases = procs.map(tokenize_frases, frases, chunksize=25)\n",
        "        print('Executando processo de remoÃ§Ã£o das stopwords...')\n",
        "        tokenized_frases = procs.map(rm_stop_words_tokenized, tokenized_frases, chunksize=25)\n",
        "    print('Filtro e finalizaÃ§Ã£o...')\n",
        "    return tokenized_frases\n",
        "\n",
        "frases = [\n",
        "    'Bom dia SENADOR, agora estÃ¡ claro porque o pedÃ¡gio nÃ£o baixava,o judiciÃ¡rio nÃ£o se manifestava quando era provocado e as CPIs sÃ³ serviram prÃ¡ corrupÃ§Ã£o,deu no que deu ðŸ™„'.split(),\n",
        "    'NÃ£o basta apenas retirar o candidato preferencial da maioria dos eleitores brasileiros. Tem que impedir tambÃ©m que esses mesmos eleitores possam comparecer Ã s urnas. Que democracia Ã© essa, minha gente? Poder judiciÃ¡rio comprometido atÃ© os cabelos com o golpe de destrÃ³i o paÃ­s.'.split(),\n",
        "    'Deus abenÃ§oe o dia de todos vocÃª, tenham um bom trabalho e bom estudo a todos. E pra aqueles que nÃ£o trabalha e nem estuda, boa curtiÃ§Ã£o em sua cama ðŸ™‚'.split(),\n",
        "    'Aprenda a ter amor prÃ³prio que nem essa banana q fez uma tatuagem dela mesma.'.split()\n",
        "]\n",
        "\n",
        "N = 10000\n",
        "# frases = flt.sents()[:N] + mch.sents()[:N] + mcm.sents()[:N]\n",
        "\n",
        "frases = generate_corpus(frases, tokenize=False)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando processamento...\n",
            "Executando processo de remoÃ§Ã£o das stopwords...\n",
            "Filtro e finalizaÃ§Ã£o...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LEUFJNpSrw9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1319
        },
        "outputId": "a08d28f5-5fe1-46de-c152-1be66f779507"
      },
      "cell_type": "code",
      "source": [
        "# d = feedparser.parse('http://rss.uol.com.br/feed/tecnologia.xml')\n",
        "# for entry in d['entries']:\n",
        "#     pp.pprint(entry['link'])\n",
        "\n",
        "stpwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "def rm_stop_words(palavra):\n",
        "    if palavra not in stpwords and not palavra.isdigit():\n",
        "        return re.sub(r'[\\\"\\'!@#$%&*\\(\\)-_=+{}\\[\\]:;>.<,|\\\\`Â´]', '', palavra.lower())\n",
        "    return None\n",
        "\n",
        "# Machado + Mac_Morpho\n",
        "print('Iniciando corpus...')\n",
        "N = 100000\n",
        "palavras = flt.words()[:N] + mch.words()[:N] + mcm.words()[:N]\n",
        "print('Corpus criado...')\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "    print('Executando processo de remoÃ§Ã£o das stopwords...')\n",
        "    palavras = procs.map(rm_stop_words, palavras, chunksize=50)\n",
        "print('Filtro e finalizaÃ§Ã£o...')\n",
        "palavras = filter(None, palavras)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando corpus...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-68e0c1738123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iniciando corpus...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpalavras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corpus criado...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collections.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazySubsequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mslice_bounds\u001b[0;34m(sequence, slice_obj, allow_step)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;31m# sequences, calculating the length of a sequence can be expensive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collections.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Use iterate_from to extract it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index out of range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m'block reader %s() should return list or tuple.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36mread_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"\"\"Reads one paragraph at a time.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpara_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_para_block_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36m_read_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mread_regexp_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'.*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'.*_\\.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTimitTaggedCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTaggedCorpusReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36mread_regexp_block\u001b[0;34m(stream, start_re, end_re)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0moldpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# End of file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mtell\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;31m# Store our original file position, so we can return here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0morig_filepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Calculate an estimate of where we think the newline is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7CXkPtuVzio8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "954108f4-6198-40dd-aa7e-d3e3051574b8"
      },
      "cell_type": "code",
      "source": [
        "print('Tf-Idf:')\n",
        "vectorizer = TfidfVectorizer(max_df=2, sublinear_tf=True, use_idf=True, ngram_range=(1, 3))\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_tfidf.get_shape()[1])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-Idf:\n",
            "   Actual number of tfidf features: 183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xIe-8XVdjjmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "08683100-d47e-4543-b619-00d0c80c2959"
      },
      "cell_type": "code",
      "source": [
        "weights = np.asarray(X_tfidf.mean(axis=0)).ravel().tolist()\n",
        "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
        "weights_df = weights_df.sort_values(by='weight', ascending=True)\n",
        "display(weights_df.head(10))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>gente</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>possam</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>comprometido cabelos golpe</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>comprometido cabelos</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>comprometido</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>comparecer urnas que</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>comparecer urnas</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>comparecer</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>possam comparecer</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>possam comparecer urnas</td>\n",
              "      <td>0.028219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           term    weight\n",
              "91                        gente  0.028219\n",
              "128                      possam  0.028219\n",
              "52   comprometido cabelos golpe  0.028219\n",
              "51         comprometido cabelos  0.028219\n",
              "50                 comprometido  0.028219\n",
              "49         comparecer urnas que  0.028219\n",
              "48             comparecer urnas  0.028219\n",
              "47                   comparecer  0.028219\n",
              "129           possam comparecer  0.028219\n",
              "130     possam comparecer urnas  0.028219"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xyW_HKxkwURE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3a73d0ec-3dd1-49c6-a9bb-7c44d39edecb"
      },
      "cell_type": "code",
      "source": [
        "print(\"LSA using TruncatedSVD:\")\n",
        "\n",
        "# Project the tfidf vectors onto the first N principal components.\n",
        "# Though this is significantly fewer features than the original tfidf vector,\n",
        "# they are stronger features, and the accuracy is higher.\n",
        "svd = TruncatedSVD(100)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_lsa = lsa.fit_transform(X_tfidf)\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(\"   Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
        "\n",
        "print(svd.explained_variance_.shape)\n",
        "print(svd.singular_values_.shape) # S\n",
        "print(svd.components_.shape) # VT"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using TruncatedSVD:\n",
            "   Explained variance of the SVD step: 100%\n",
            "(4,)\n",
            "(4,)\n",
            "(4, 183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHnVMukg0oWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4ec046ea-af86-42ce-c3dd-22b8ce02c39a"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using numpy:')\n",
        "u, s, v = np.linalg.svd(X_tfidf.toarray(), full_matrices=False)\n",
        "print(u.shape)\n",
        "print(s.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using numpy:\n",
            "(4, 4)\n",
            "(4,)\n",
            "(4, 183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_KCInH7aaUQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b38d61a8-186d-4aac-8b8d-a9b85099557b"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using scikit-learn randomized_svd:')\n",
        "U, Sigma, VT = randomized_svd(X_tfidf, \n",
        "                              n_components=100,\n",
        "                              n_iter=5,\n",
        "                              random_state=None)\n",
        "print(U.shape)\n",
        "print(Sigma.shape)\n",
        "print(VT.shape)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using scikit-learn randomized_svd:\n",
            "(4, 4)\n",
            "(4,)\n",
            "(4, 183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xy-B3w6obyOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}