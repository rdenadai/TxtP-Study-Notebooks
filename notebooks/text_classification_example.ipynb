{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/TxtP-Study-Notebooks/blob/master/notebooks/text_classification_example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WGxO__uG47K5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## AnÃ¡lise e ValidaÃ§Ã£o de Textos em PortuguÃªs\n"
      ]
    },
    {
      "metadata": {
        "id": "AzVNkORP5dvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ReferÃªncias:\n",
        "\n",
        " - Bibliotecas:\n",
        "  - [NLTK](http://www.nltk.org/howto/portuguese_en.html)\n",
        "  - [spaCy](https://spacy.io/usage/spacy-101)\n",
        " \n",
        " - Tutoriais\n",
        "  - [Utilizando processamento de linguagem natural para criar uma sumarizaÃ§Ã£o automÃ¡tica de textos](https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e)\n",
        "  - [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
        "  - [Machine Learning :: Cosine Similarity for Vector Space Models (Part III)](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)\n",
        "  - [My Notes for Singular Value Decomposition with Interactive Code ](https://towardsdatascience.com/my-notes-for-singular-value-decomposition-with-interactive-code-feat-peter-mills-7584f4f2930a)\n",
        "  - [https://plot.ly/ipython-notebooks/principal-component-analysis/](https://plot.ly/ipython-notebooks/principal-component-analysis/)\n",
        " \n",
        " - Topic Modelling\n",
        "  - [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
        "  - [Integrating Topics and Syntax (HHM-LDA)](http://psiexp.ss.uci.edu/research/papers/composite.pdf)\n",
        " \n",
        " - Others\n",
        "  - [Um MÃ©todo de IdentificaÃ§Ã£o de EmoÃ§Ãµes em Textos Curtos para o PortuguÃªs do Brasil](http://www.ppgia.pucpr.br/~paraiso/Projects/Emocoes/Emocoes.html)\n",
        "  - [An Introduction to Latent Semantic Analysis](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf)\n",
        "  - [Unsupervised Emotion Detection from Text using Semantic and Syntactic Relations](http://www.cse.yorku.ca/~aan/research/paper/Emo_WI10.pdf)\n",
        "  - [An Efficient Method for Document Categorization Based on Word2vec and Latent Semantic Analysis](https://ieeexplore.ieee.org/document/7363382)\n",
        "  - [Sentiment Classification of Documents Based on Latent Semantic Analysis](https://link.springer.com/chapter/10.1007/978-3-642-21802-6_57)\n",
        "  - [Applying latent semantic analysis to classify emotions in Thai text](https://ieeexplore.ieee.org/document/5486137)\n",
        "  - [Text Emotion Classification Research Based on Improved Latent Semantic Analysis Algorithm](https://www.researchgate.net/publication/266651993_Text_Emotion_Classification_Research_Based_on_Improved_Latent_Semantic_Analysis_Algorithm)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fJl2_xgi5M4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### InstalaÃ§Ã£o"
      ]
    },
    {
      "metadata": {
        "id": "6gSt861Txvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "outputId": "79f08592-6c9d-46d9-a753-396775170513"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt\n",
        "# !pip install feedparser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.4MB 20.6MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz (38.7MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.7MB 58.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "  Running setup.py install for pt-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed pt-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PcT7xbKKo55-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "204411ec-1812-47dd-9e39-134f70dffa82"
      },
      "cell_type": "code",
      "source": [
        "# Download Oplexicon\n",
        "!rm -rf wget-log*\n",
        "!rm -rf oplexicon_v3.0\n",
        "!wget -O oplexicon_v3.0.zip https://github.com/rdenadai/sentiment-analysis-2018-president-election/blob/master/dataset/oplexicon_v3.0.zip?raw=true\n",
        "!unzip oplexicon_v3.0.zip\n",
        "!ls -lh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to â€˜wget-logâ€™.\n",
            "Archive:  oplexicon_v3.0.zip\n",
            "  inflating: oplexicon_v3.0/lexico_v3.0.txt  \n",
            "  inflating: oplexicon_v3.0/README   \n",
            "total 120K\n",
            "drwxr-xr-x 2 root root 4.0K Oct  9 16:00 oplexicon_v3.0\n",
            "-rw-r--r-- 1 root root 102K Oct  9 16:00 oplexicon_v3.0.zip\n",
            "drwxr-xr-x 2 root root 4.0K Sep 28 23:32 sample_data\n",
            "-rw-r--r-- 1 root root 1.6K Oct  9 16:00 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVxM4zTo5QSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HLlEa6uEyX11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "0c049133-d603-43e3-f7d8-cc2257282986"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('floresta')\n",
        "nltk.download('mac_morpho')\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import concurrent.futures\n",
        "import codecs\n",
        "import re\n",
        "import pprint\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "import copy\n",
        "from unicodedata import normalize\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.linalg import svd\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.pt.lemmatizer import LOOKUP\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import floresta as flt\n",
        "from nltk.corpus import machado as mch\n",
        "from nltk.corpus import mac_morpho as mcm\n",
        "\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/floresta.zip.\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0CGK0ftshPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ]
    },
    {
      "metadata": {
        "id": "pOMbwFd3sd31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalization(x, a, b):\n",
        "    return (2 * b) * (x - np.min(x)) / np.ptp(x) + a\n",
        "\n",
        "\n",
        "def remover_acentos(txt):\n",
        "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "\n",
        "def load_oplexicon_data(filename):\n",
        "    spacy_conv = {\n",
        "        'adj': 'ADJ',\n",
        "        'n': 'NOUN',\n",
        "        'vb': 'VERB',\n",
        "        'det': 'DET',\n",
        "        'emot': 'EMOT',\n",
        "        'htag': 'HTAG'\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    with codecs.open(filename, 'r', 'UTF-8') as hf:\n",
        "        lines = hf.readlines()\n",
        "        for line in lines:\n",
        "            info = line.lower().split(',')\n",
        "            if len(info[0].split()) <= 1:\n",
        "                info[1] = [spacy_conv.get(tag) for tag in info[1].split()]\n",
        "                word, tags, sent = info[:3]\n",
        "                if 'HTAG' not in tags and 'EMOT' not in tags:\n",
        "                    word = remover_acentos(word.replace('-se', ''))\n",
        "                    word = LOOKUP.get(word, word)\n",
        "                    # stem = stemmer.stem(word)\n",
        "                    if word in data:\n",
        "                        data[word] += [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "                    else:\n",
        "                        data[word] = [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwSQhPMk5Scp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ]
    },
    {
      "metadata": {
        "id": "mQzbtaNhy5Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "7e0229c0-ec9f-4e44-b198-193c39708a9b"
      },
      "cell_type": "code",
      "source": [
        "frase = u\"Gostaria de saber mais informaÃ§Ãµes sobre a Amazon. Uma excelente loja de produtos online!\".lower()\n",
        "doc = nlp(remover_acentos(frase))\n",
        "pp.pprint([(w.text, w.pos_) for w in doc])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   ('gostaria', 'VERB'),\n",
            "    ('de', 'ADP'),\n",
            "    ('saber', 'VERB'),\n",
            "    ('mais', 'ADV'),\n",
            "    ('informacoes', 'NOUN'),\n",
            "    ('sobre', 'ADP'),\n",
            "    ('a', 'DET'),\n",
            "    ('amazon', 'NOUN'),\n",
            "    ('.', 'PUNCT'),\n",
            "    ('uma', 'DET'),\n",
            "    ('excelente', 'ADJ'),\n",
            "    ('loja', 'NOUN'),\n",
            "    ('de', 'ADP'),\n",
            "    ('produtos', 'NOUN'),\n",
            "    ('online', 'ADJ'),\n",
            "    ('!', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F--aytOy3gL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "e4de5add-9ab6-4477-a34c-c7626c7ce66c"
      },
      "cell_type": "code",
      "source": [
        "opx = load_oplexicon_data('oplexicon_v3.0/lexico_v3.0.txt')\n",
        "print('Oplexicon size: ', len(opx))\n",
        "print('Examples: ')\n",
        "\n",
        "view = opx.items()\n",
        "pp.pprint(list(view)[:7])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oplexicon size:  15958\n",
            "Examples: \n",
            "[   ('ab-rogar', [{'sentiment': '-1', 'tags': ['VERB'], 'word': ['ab-rogar']}]),\n",
            "    ('ababadar', [{'sentiment': '0', 'tags': ['VERB'], 'word': ['ababadar']}]),\n",
            "    (   'ababelar',\n",
            "        [   {'sentiment': '-1', 'tags': ['VERB'], 'word': ['ababelar']},\n",
            "            {'sentiment': '1', 'tags': ['VERB'], 'word': ['ababelar']}]),\n",
            "    ('abacanar', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abacanar']}]),\n",
            "    ('abacinar', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abacinar']}]),\n",
            "    (   'abafar',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafar']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafar']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafar']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafar']},\n",
            "            {'sentiment': '-1', 'tags': ['VERB'], 'word': ['abafar']}]),\n",
            "    (   'abafante',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafante']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafante']}])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AcajLBiFm3N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ALEGRIA = ['abundante', 'acalmar', 'aceitÃ¡vel', 'aclamar', 'aconchego', 'adesÃ£o', 'admirar', 'adorar', 'afÃ¡vel', 'afeiÃ§Ã£o', 'afeto', 'afortunado', 'agradar', 'ajeitar', 'alÃ­vio', 'amabilidade', 'amado', 'amar', 'amÃ¡vel', 'amenizar', 'ameno', 'amigÃ¡vel', 'amistoso', ' amizade', ' amor', ' animaÃ§Ã£o', ' Ã¢nimo', 'anseio', 'Ã¢nsia', 'ansioso', 'apaixonado', 'apaziguar', 'aplausos', 'apoiar', 'aprazer', 'apreciar', 'aprovaÃ§Ã£o', 'aproveitar', 'ardor', 'armirar', 'arrumar', 'atraÃ§Ã£o', 'atraente', 'atrair', 'avidamente', 'avidez', 'Ã¡vido', 'belo', 'bem-estar', 'beneficÃªncia', 'beneficiador', 'benefÃ­cio', 'benÃ©fico', 'benevocÃªncia', 'benignamente', 'benÃ­gno', 'bom', 'bondade', 'bondoso', 'bonito', 'brilhante', 'brincadeira', 'calma', 'calor', 'caridade', 'caridoso', 'carinho', 'cativar', 'charme', 'cheery', 'clamar', 'cofortar', 'coleguismo', 'comÃ©dia', 'cÃ´mico', 'comover', 'compaixÃ£o', 'companheirismo', 'compatibilidade', 'compatÃ­vel', 'complacÃªncia', 'completar', 'compreensÃ£o', 'conclusÃ£o', 'concretizaÃ§Ã£o', 'condescendÃªncia', 'confianÃ§a', 'confortante', 'congratulaÃ§Ã£o', 'conquistar', 'consentir', 'consideraÃ§Ã£o', 'consolaÃ§Ã£o', 'contentamento', 'coragem', 'cordial', 'considerar', 'consolo', 'contente', 'cuidadoso', 'cumplicidade', 'dedicaÃ§Ã£o', 'deleitado', 'delicadamente', 'delicadeza', 'delicado', 'desejar', 'despreocupaÃ§Ã£o', 'devoÃ§Ã£o', 'devoto', 'diversÃ£o', 'divertido', 'encantar', 'elogiado', 'emoÃ§Ã£o', 'emocionante', 'emotivo', 'empatia', 'empÃ¡tico', 'empolgaÃ§Ã£o', 'enamorar', 'encantado', 'encorajado', 'enfeitar', 'engraÃ§ado', 'entendimento', 'entusiasmadamente', 'entusiÃ¡stico', 'esperanÃ§a', 'esplendor', 'estima', 'estimar', 'estimulante', 'euforia', 'eufÃ³rico', 'euforizante', 'exaltar', 'excelente', 'excitar', 'expansivo', 'extasiar', 'exuberante', 'exultar', 'fÃ£', 'facilitar', 'familiaridade', 'fascinaÃ§Ã£o', 'fascÃ­nio', 'favor', 'favorecer', 'favorito', 'felicidade', 'feliz', 'festa', 'festejar', 'festivo', 'fidelidade', 'fiel', 'filantropia', 'filantrÃ³pico', 'fraterno', 'ganhar', 'generosidade', 'generoso', 'gentil', 'glÃ³ria', 'glorificar', 'gostar', 'gostoso', 'gozar', 'gratificante', 'grato', 'hilariante', 'honra', 'humor', 'impressionar', 'incentivar', 'incentivo', 'inclinaÃ§Ã£o', 'incrÃ­vel', 'inspirar', 'interessar', 'interesse', 'irmandade', 'jovial', 'jubilante', 'jÃºbilo', 'lealdade', 'legÃ­timo', 'leveza', 'louvar', 'louvÃ¡vel', 'louvavelmente', 'lucrativo', 'lucro', 'maravilhoso', 'melhor', 'obter', 'obteve', 'ode', 'orgulho', 'paixÃ£o', 'parabenizar', 'paz', 'piedoso', 'positivo', 'prazenteiro', 'prazer', 'predileÃ§Ã£o', 'preencher', 'preferÃªncia', 'preferido', 'promissor', 'prosperidade', 'proteÃ§Ã£o', 'proteger', 'revigorar', 'simpÃ¡tico', 'vantajoso', 'protetor', 'risada', 'sobrevivÃªncia', 'vencedor', 'proveito', 'risonho', 'sobreviver', 'veneraÃ§Ã£o', 'provilÃ©gio', 'romÃ¢ntico', 'sorte', 'ventura', 'querer', 'romantismo', 'sortudo', 'vida', 'radiante', 'saciar', 'sucesso', 'vigor', 'realizar', 'saciÃ¡vel', 'surpreender', 'virtude', 'recomendÃ¡vel', 'satisfaÃ§Ã£o', 'tenro', 'virtuoso', 'reconhecer', 'satisfatoriamente', 'ternura', 'vitÃ³ria', 'recompensa', 'satisfatÃ³rio', 'torcer', 'vitorioso', 'recrear', 'satisfazer', 'tranquilo', 'viver', 'recreativo', 'satisfeito', 'tranquilo', 'vivo', 'recreaÃ§Ã£o', 'seduÃ§Ã£o', 'triunfo', 'zelo', 'regozijar', 'seduzir', 'triunfal', 'zeloso', 'respeitar', 'sereno', 'triunfante', 'ressuscitar', 'simpaticamente', 'vantagem',]\n",
        "DESGOSTO = ['abominÃ¡vel', 'adoentado', 'amargamente', 'antipatia', 'antipÃ¡tico', 'asco', 'asqueroso', 'aversÃ£o', 'chatear', 'chateaÃ§Ã£o', 'desagrado', 'desagradÃ¡vel', 'desprezÃ­vel', 'detestÃ¡vel', 'doente', 'doenÃ§a', 'enfermidade', 'enjoativo', 'enjoo', 'enjÃ´o', 'feio', 'fÃ©tido', 'golfar', 'grave', 'gravidade', 'grosseiro', 'grosso', 'horrÃ­vel', 'ignÃ³bil', 'ilegal', 'incomodar', 'incÃ´mdo', 'indecente', 'indisposiÃ§Ã£o', 'indisposto', 'inescrupuloso', 'maldade', 'maldoso', 'malvado', 'mau', 'nauseabundo', 'nauseante', 'nausear', 'nauseoso', 'nojento', 'nojo', 'nÃ¡usea', 'obsceno', 'obstruir', 'obstruÃ§Ã£o', 'ofensivo', 'patÃ©tico', 'perigoso', 'repelente', 'repelir', 'repugnante', 'repulsa', 'repulsivo', 'repulsÃ£o', 'rude', 'sujeira', 'sujo', 'terrivelmente', 'terrÃ­vel', 'torpe', 'travesso', 'travessura', 'ultrajante', 'vil', 'vomitar', 'vÃ´mito',]\n",
        "MEDO = ['abominÃ¡vel', 'afugentar', 'alarmar', 'alerta', 'ameaÃ§a', 'amedrontar', 'angustia', 'angÃºstia', 'angustiadamente', 'ansiedade', 'ansioso', 'apavorar', 'apreender', 'apreensÃ£o', 'apreensivo', 'arrepio', 'assombrado', 'assombro', 'assustado', 'assustadoramente', 'atemorizar', 'aterrorizante', 'brutal', 'calafrio', 'chocado', 'chocante', 'consternado', 'covarde', 'cruel', 'crueldade', 'cruelmente', 'cuidado', 'cuidadosamente', 'cuidadoso', 'defender', 'defensor', 'defesa', 'derrotar', 'desconfiado', 'desconfianÃ§a', 'desencorajar', 'desespero', 'deter', 'envergonhado', 'escandalizado', 'escuridÃ£o', 'espantoso', 'estremecedor', 'estremecer', 'expulsar', 'feio', 'friamente', 'fugir', 'hesitar', 'horrendo', 'horripilante', 'horrÃ­vel', 'horrivelmente', 'horror', 'horrorizar', 'impaciÃªncia', 'impaciente', 'impiedade', 'impiedoso', 'indecisÃ£o', 'inquieto', 'inseguranÃ§a', 'inseguro', 'intimidar', 'medonho', 'medroso', 'monstruosamente', 'mortalha', 'nervoso', 'pÃ¢nico', 'pavor', 'premoniÃ§Ã£o', 'preocupar', 'pressÃ¡gio', 'pressentimento', 'recear', 'receativamente', 'receio', 'receoso', 'ruim', 'suspeita', 'suspense', 'susto', 'temer', 'tenso', 'terror', 'tremor', 'temeroso', 'terrificar', 'timidamente', 'vigiar', 'temor', 'terrÃ­vel', 'timidez', 'vigilante', 'tensÃ£o', 'terrivelmente', 'tÃ­mido',]\n",
        "RAIVA = ['abominaÃ§Ã£o', 'aborrecer', 'adredido', 'agredir', 'agressÃ£o', 'agressivo', 'amaldiÃ§oado', 'amargor', 'amargura', 'amolar', 'angÃºstia', 'animosidade', 'antipatia', 'antipÃ¡tico', 'asco', 'assassinar', 'assassinato', 'assediar', 'assÃ©dio', 'atormentar', 'avarento', 'avareza', 'aversÃ£o', 'beligerante', 'bravejar', 'chateaÃ§Ã£o', 'chato', 'cobiÃ§oso', 'cÃ³lera', 'colÃ©rico', 'complicar', 'contraiedade', 'contrariar', 'corrupÃ§Ã£o', 'corrupto', 'cruxificar', 'demonÃ­aco', 'demÃ´nio', 'descaso', 'descontente', 'descontrole', 'desenganar', 'desgostar', 'desgraÃ§a', 'desprazer', 'desprezar', 'destruiÃ§Ã£o', 'destruir', 'detestar', 'diabo', 'diabÃ³lico', 'doido', 'encolerizar', 'energicamente', 'enfurecido', 'enfuriante', 'enlouquecer', 'enraivecer', 'escandalizar', 'escÃ¢ndalo', 'escoriar', 'exasperar', 'execraÃ§Ã£o', 'ferir', 'frustraÃ§Ã£o', 'frustrar', 'fÃºria', 'furioso', 'furor', 'ganÃ¢ncia', 'ganancioso', 'guerra', 'guerreador', 'guerrilha', 'hostil', 'humilhar', 'implicÃ¢ncia', 'implicar', 'importunar', 'incomodar', 'incÃ´modo', 'indignar', 'infernizar', 'inimigo', 'inimizade', 'injÃºria', 'injuriado', 'injustiÃ§a', 'insulto', 'malÃ­cia', 'odiÃ¡vel', 'repulsivo', 'inveja', 'malicioso', 'Ã³dio', 'resmungar', 'ira', 'malignidade', 'odioso', 'ressentido', 'irado', 'malÃ­gno', 'ofendido', 'revolta', 'irascibilidade', 'maltratar', 'ofensa', 'ridÃ­culo', 'irascÃ­vel', 'maluco', 'opressÃ£o', 'tempestuoso', 'irritar', 'malvadeza', 'opressivo', 'tirano', 'louco', 'malvado', 'oprimir', 'tormento', 'loucura', 'matar', 'perseguiÃ§Ã£o', 'torturar', 'magoar', 'mesquinho', 'perseguir', 'ultrage', 'mal', 'misantropia', 'perturbar', 'ultrajar', 'maldade', 'misantrÃ³pico', 'perverso', 'vexatÃ³rio', 'maldiÃ§Ã£o', 'molestar', 'provocar', 'vigoroso', 'maldito', 'molÃ©stia', 'rabugento', 'vinganÃ§a', 'maldizer', 'mortal', 'raivoso', 'vingar', 'maldoso', 'morte', 'rancor', 'vingativo', 'maleficÃªncia', 'mortÃ­fero', 'reclamar', 'violÃªncia', 'malÃ©fico', 'mortificar', 'repressÃ£o', 'violento', 'malevolÃªncia', 'nervoso', 'reprimir', 'zangar', 'malÃ©volo', 'odiar', 'repulsa',]\n",
        "SURPRESA = ['admirar', 'afeiÃ§Ã£o', 'apavorante', 'assombro', 'chocado', 'chocante', 'desconcertar', 'deslumbrar', 'embasbacar', 'emudecer', 'encantamento', 'enorme', 'espanto', 'estupefante', 'estupefato', 'estupefazer', 'expectativa', 'fantasticamente', 'fantÃ¡stico', 'horripilante', 'imaginÃ¡rio', 'imenso', 'impressionado', 'incrÃ­vel', 'maravilha', 'milagre', 'mistÃ©rio', 'misterioso', 'Ã³timo', 'pasmo', 'perplexo', 'prodÃ­gio', 'sensacional', 'surpreendente', 'surpreender', 'suspense', 'susto', 'temor', 'tremendo',]\n",
        "TRISTEZA = ['abandonar', 'abatido', 'abominÃ¡vel', 'aborrecer', 'abortar', 'afligir', 'aflito', 'afliÃ§Ã£o', 'agoniar', 'amargo', 'amargor', 'amargura', 'ansiedade', 'arrepender', 'arrependidamente', 'atrito', 'azar', 'cabisbaixo', 'choro', 'choroso', 'chorÃ£o', 'coitado', 'compassivo', 'compunÃ§Ã£o', 'contristador', 'contrito', 'contriÃ§Ã£o', 'culpa', 'defeituoso', 'degradante', 'deplorÃ¡vel', 'deposiÃ§Ã£o', 'depravado', 'depressivo', 'depressÃ£o', 'deprimente', 'deprimir', 'derrota', 'derrubar', 'desalentar', 'desamparo', 'desanimar', 'desapontar', 'desconsolo', 'descontente', 'desculpas', 'desencorajar', 'desespero', 'desgaste', 'desgosto', 'desgraÃ§a', 'desistir', 'desistÃªncia', 'deslocado', 'desmoralizar', 'desolar', 'desonra', 'despojado', 'desprazer', 'desprezo', 'desumano', 'desÃ¢nimo', 'discriminar', 'disforia', 'disfÃ³rico', 'dissuadir', 'doloroso', 'dor', 'dÃ³', 'enfadado', 'enlutar', 'entediado', 'entristecedor', 'entristecer', 'envergonhar', 'errante', 'erro', 'errÃ´neo', 'escurecer', 'escuridÃ£o', 'escuro', 'esquecido', 'estragado', 'execrÃ¡vel', 'extirpar', 'falsidade', 'falso', 'falta', 'fraco', 'fraqueza', 'fricÃ§Ã£o', 'frieza', 'frio', 'funesto', 'fÃºnebre', 'grave', 'horror', 'humilhar', 'inconsolÃ¡vel', 'indefeso', 'infelicidade', 'infeliz', 'infortÃºnio', 'isolar', 'lacrimejante', 'lacrimoso', 'lamentar', 'lastimoso', 'luto', 'lutoso', 'lÃ¡grima', 'lÃ¡stima', 'lÃºgubre', 'magoar', 'martirizar', 'martÃ­rio', 'mau', 'melancolia', 'melancÃ³lico', 'menosprezar', 'miseravelmente', 'misterioso', 'mistÃ©rio', 'misÃ©ria', 'morre', 'morte', 'mortificante', 'mÃ¡goa', 'negligentemente', 'nocivo', 'obscuro', 'opressivo', 'opressÃ£o', 'oprimir', 'pena', 'penalizar', 'penitente', 'penoso', 'penumbra', 'perder', 'perturbado', 'perverso', 'pervertar', 'pesaroso', 'pessimamente', 'piedade', 'pobre', 'porcamente', 'prejudicado', 'prejudicial', 'prejuÃ­zo', 'pressionar', 'pressÃ£o', 'quebrar', 'queda', 'queixoso', 'rechaÃ§ar', 'remorso', 'repressivo', 'repressÃ£o', 'reprimir', 'ruim', 'secreto', 'servil', 'sobrecarga', 'sobrecarregado', 'sofrer', 'sofrimento', 'solidÃ£o', 'sombrio', 'soturno', 'sujo', 'suplicar', 'suplÃ­cio', 'sÃ³', 'timidez', 'torturar', 'trevas', 'triste', 'tristemente', 'tÃ©dio', 'tÃ­mido', 'vazio',]\n",
        "\n",
        "emotion_words = {\n",
        "    'ALEGRIA': ALEGRIA,\n",
        "    'DESGOSTO': DESGOSTO,\n",
        "    'MEDO': MEDO,\n",
        "    'RAIVA': RAIVA,\n",
        "    'SURPRESA': SURPRESA,\n",
        "    'TRISTEZA': TRISTEZA,\n",
        "}\n",
        "for key, values in emotion_words.items():\n",
        "    for i, word in enumerate(values):\n",
        "        emotion_words[key][i] = ''.join([remover_acentos(p.strip()) for p in LOOKUP.get(word.lower(), word.lower())])\n",
        "# pp.pprint(emotion_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zf-PxyeIwi0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7fa496a3-40d7-4f89-ba26-075cfd6a879e"
      },
      "cell_type": "code",
      "source": [
        "stpwords = stopwords.words('portuguese') + list(punctuation)\n",
        "rms = ['um', 'nÃ£o', 'mais', 'muito']\n",
        "for rm in rms:\n",
        "    del stpwords[stpwords.index(rm)]\n",
        "\n",
        "def tokenize_frases(frase):\n",
        "    return word_tokenize(remover_acentos(frase.lower()))\n",
        "\n",
        "def rm_stop_words_tokenized(frase):\n",
        "    frase = nlp(remover_acentos(frase.lower()))\n",
        "    clean_frase = []\n",
        "    for palavra in frase:\n",
        "        if palavra.pos_ != 'PUNCT':\n",
        "            palavra = palavra.lemma_\n",
        "            for pc in ['.', ',']:\n",
        "                palavra = palavra.replace(pc, '')\n",
        "            if palavra not in stpwords and not palavra.isdigit():\n",
        "                clean_frase.append(palavra)\n",
        "    return ' '.join(clean_frase)\n",
        "\n",
        "def generate_corpus(frases, tokenize=False):\n",
        "    print('Iniciando processamento...')\n",
        "    tokenized_frases = frases\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "        if tokenize:\n",
        "            print('Executando processo de tokenizaÃ§Ã£o das frases...')\n",
        "            tokenized_frases = procs.map(tokenize_frases, frases, chunksize=25)\n",
        "        print('Executando processo de remoÃ§Ã£o das stopwords...')\n",
        "        tokenized_frases = procs.map(rm_stop_words_tokenized, tokenized_frases, chunksize=25)\n",
        "    print('Filtro e finalizaÃ§Ã£o...')\n",
        "    return tokenized_frases\n",
        "\n",
        "\n",
        "frases = [\n",
        "    'Bom dia SENADOR, agora estÃ¡ claro porque o pedÃ¡gio nÃ£o baixava,o judiciÃ¡rio nÃ£o se manifestava quando era provocado e as CPIs sÃ³ serviram prÃ¡ corrupÃ§Ã£o,deu no que deu ðŸ™„',\n",
        "    'NÃ£o basta apenas retirar o candidato preferencial da maioria dos eleitores brasileiros. Tem que impedir tambÃ©m que esses mesmos eleitores possam comparecer Ã s urnas. Que democracia Ã© essa, minha gente? Poder judiciÃ¡rio comprometido atÃ© os cabelos com o golpe de destrÃ³i o paÃ­s.',\n",
        "    'Deus abenÃ§oe o dia de todos vocÃª, tenham um bom trabalho e bom estudo a todos. E pra aqueles que nÃ£o trabalha e nem estuda, boa curtiÃ§Ã£o em sua cama ðŸ™‚',\n",
        "    'Aprenda a ter amor prÃ³prio que nem essa banana q fez uma tatuagem dela mesma.',\n",
        "    'Estou muito feliz hoje',\n",
        "    'Dias chuvosos me deixam triste',\n",
        "    'Hoje o dia esta excelente',\n",
        "    'Tem certas coisas que eu nÃ£o como, acho bem nojento ficar mastigando lingua de boi por exemplo',\n",
        "    'Ã‰ de se admirar Ã queles que conseguem realizar boas aÃ§Ãµes sem desejar nada em troca',\n",
        "]\n",
        "\n",
        "# N = 10000\n",
        "\n",
        "# frases += [' '.join(f).replace('_', ' ') for f in flt.sents()[:250]]\n",
        "# frases += [' '.join(f).replace('_', ' ') for f in mch.sents()[:250]]\n",
        "# frases += [' '.join(f).replace('_', ' ') for f in mcm.sents()[:250]]\n",
        "\n",
        "frases = list(generate_corpus(frases, tokenize=False))\n",
        "# print(frases)\n",
        "\n",
        "ldocs = [f'D{i}' for i in range(len(frases))]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando processamento...\n",
            "Executando processo de remoÃ§Ã£o das stopwords...\n",
            "Filtro e finalizaÃ§Ã£o...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7CXkPtuVzio8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "ab79f866-9564-4e10-f598-e4b449b5d20d"
      },
      "cell_type": "code",
      "source": [
        "print('Tf-Idf:')\n",
        "vec_tfidf = TfidfVectorizer(max_df=5, sublinear_tf=False, use_idf=True, ngram_range=(1, 3))\n",
        "X_tfidf = vec_tfidf.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_tfidf.get_shape()[1])\n",
        "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 3), index=vec_tfidf.get_feature_names(), columns=ldocs)\n",
        "display(weights_tfidf.head(15))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-Idf:\n",
            "   Actual number of tfidf features: 271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abencoe</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abencoe dia</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abencoe dia todo</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achar bem</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achar bem nojento</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acoes</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acoes desejar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acoes desejar nado</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admirar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admirar conseguir</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admirar conseguir realizar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>0.129</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora claro</th>\n",
              "      <td>0.129</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora claro porque</th>\n",
              "      <td>0.129</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               D0   D1     D2   D3   D4   D5   D6     D7  \\\n",
              "abencoe                     0.000  0.0  0.126  0.0  0.0  0.0  0.0  0.000   \n",
              "abencoe dia                 0.000  0.0  0.126  0.0  0.0  0.0  0.0  0.000   \n",
              "abencoe dia todo            0.000  0.0  0.126  0.0  0.0  0.0  0.0  0.000   \n",
              "achar                       0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.169   \n",
              "achar bem                   0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.169   \n",
              "achar bem nojento           0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.169   \n",
              "acoes                       0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "acoes desejar               0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "acoes desejar nado          0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "admirar                     0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "admirar conseguir           0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "admirar conseguir realizar  0.000  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "agora                       0.129  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "agora claro                 0.129  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "agora claro porque          0.129  0.0  0.000  0.0  0.0  0.0  0.0  0.000   \n",
              "\n",
              "                               D8  \n",
              "abencoe                     0.000  \n",
              "abencoe dia                 0.000  \n",
              "abencoe dia todo            0.000  \n",
              "achar                       0.000  \n",
              "achar bem                   0.000  \n",
              "achar bem nojento           0.000  \n",
              "acoes                       0.221  \n",
              "acoes desejar               0.221  \n",
              "acoes desejar nado          0.221  \n",
              "admirar                     0.221  \n",
              "admirar conseguir           0.221  \n",
              "admirar conseguir realizar  0.221  \n",
              "agora                       0.000  \n",
              "agora claro                 0.000  \n",
              "agora claro porque          0.000  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GbRzyK7Zn4G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "2e5eda0d-e592-4eb3-c434-d18fb7104dfd"
      },
      "cell_type": "code",
      "source": [
        "print('Count:')\n",
        "vec_count = CountVectorizer()\n",
        "X_count = vec_count.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_count.get_shape()[1])\n",
        "weights_count = pd.DataFrame(X_count.toarray().T, index=vec_count.get_feature_names(), columns=ldocs)\n",
        "display(weights_count.head(15))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            "   Actual number of tfidf features: 84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abencoe</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achar</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acoes</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admirar</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>atar</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bem</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>boi</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bom</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          D0  D1  D2  D3  D4  D5  D6  D7  D8\n",
              "abencoe    0   0   1   0   0   0   0   0   0\n",
              "achar      0   0   0   0   0   0   0   1   0\n",
              "acoes      0   0   0   0   0   0   0   0   1\n",
              "admirar    0   0   0   0   0   0   0   0   1\n",
              "agora      1   0   0   0   0   0   0   0   0\n",
              "amor       0   0   0   1   0   0   0   0   0\n",
              "apenas     0   1   0   0   0   0   0   0   0\n",
              "aprender   0   0   0   1   0   0   0   0   0\n",
              "atar       0   1   0   0   0   0   0   0   0\n",
              "baixar     1   0   0   0   0   0   0   0   0\n",
              "banana     0   0   0   1   0   0   0   0   0\n",
              "basto      0   1   0   0   0   0   0   0   0\n",
              "bem        0   0   0   0   0   0   0   1   0\n",
              "boi        0   0   0   0   0   0   0   1   0\n",
              "bom        1   0   3   0   0   0   0   0   1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uvfJwFa8atEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e551779-d4ea-4f14-9c5f-e0b212e1d09d"
      },
      "cell_type": "code",
      "source": [
        "USE = True\n",
        "X = X_count if USE else X_tfidf\n",
        "weights = weights_count if USE else weights_tfidf\n",
        "vectorizer = vec_count if USE else vec_tfidf\n",
        "\n",
        "print('SVD: ')\n",
        "AC = copy.deepcopy(X.toarray().T)\n",
        "u, s, v = np.linalg.svd(AC, full_matrices=False)\n",
        "print('Original and SVD equals: ', np.allclose(AC, np.dot(u, np.dot(np.diag(s), v))))\n",
        "\n",
        "# print(AC)\n",
        "# print(u.astype(np.float16))\n",
        "# print('-' * 20)\n",
        "# print(np.diag(s.astype(np.float16)))\n",
        "# print('-' * 20)\n",
        "# print(v.astype(np.float16))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVD: \n",
            "Original and SVD equals:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "86EgpEoAJCEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "cfbcb012-4860-476e-d7d4-9c6d4917b095"
      },
      "cell_type": "code",
      "source": [
        "print('Matriz U:')\n",
        "weights_um = pd.DataFrame(u, index=vectorizer.get_feature_names(), columns=ldocs)\n",
        "display(weights_um.head(15))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz U:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abencoe</th>\n",
              "      <td>-0.121141</td>\n",
              "      <td>-0.078354</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>-0.015278</td>\n",
              "      <td>0.005196</td>\n",
              "      <td>0.044912</td>\n",
              "      <td>0.016150</td>\n",
              "      <td>-0.011787</td>\n",
              "      <td>-0.008395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achar</th>\n",
              "      <td>-0.020953</td>\n",
              "      <td>0.009936</td>\n",
              "      <td>0.010063</td>\n",
              "      <td>0.272938</td>\n",
              "      <td>-0.065397</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>-0.008330</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>0.003251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acoes</th>\n",
              "      <td>-0.014540</td>\n",
              "      <td>-0.011745</td>\n",
              "      <td>-0.010624</td>\n",
              "      <td>-0.015537</td>\n",
              "      <td>-0.137360</td>\n",
              "      <td>-0.330978</td>\n",
              "      <td>-0.022575</td>\n",
              "      <td>0.012702</td>\n",
              "      <td>0.006081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admirar</th>\n",
              "      <td>-0.014540</td>\n",
              "      <td>-0.011745</td>\n",
              "      <td>-0.010624</td>\n",
              "      <td>-0.015537</td>\n",
              "      <td>-0.137360</td>\n",
              "      <td>-0.330978</td>\n",
              "      <td>-0.022575</td>\n",
              "      <td>0.012702</td>\n",
              "      <td>0.006081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>-0.081228</td>\n",
              "      <td>-0.011599</td>\n",
              "      <td>0.188272</td>\n",
              "      <td>-0.026692</td>\n",
              "      <td>0.014662</td>\n",
              "      <td>-0.002089</td>\n",
              "      <td>0.021458</td>\n",
              "      <td>-0.015946</td>\n",
              "      <td>-0.011367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>-0.006683</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>-0.010784</td>\n",
              "      <td>0.049210</td>\n",
              "      <td>0.323950</td>\n",
              "      <td>-0.139611</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>-0.062292</td>\n",
              "      <td>0.167098</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>-0.027951</td>\n",
              "      <td>-0.011140</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>-0.003128</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>0.001563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>-0.006683</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>-0.010784</td>\n",
              "      <td>0.049210</td>\n",
              "      <td>0.323950</td>\n",
              "      <td>-0.139611</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>atar</th>\n",
              "      <td>-0.062292</td>\n",
              "      <td>0.167098</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>-0.027951</td>\n",
              "      <td>-0.011140</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>-0.003128</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>0.001563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>-0.081228</td>\n",
              "      <td>-0.011599</td>\n",
              "      <td>0.188272</td>\n",
              "      <td>-0.026692</td>\n",
              "      <td>0.014662</td>\n",
              "      <td>-0.002089</td>\n",
              "      <td>0.021458</td>\n",
              "      <td>-0.015946</td>\n",
              "      <td>-0.011367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>-0.006683</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>-0.010784</td>\n",
              "      <td>0.049210</td>\n",
              "      <td>0.323950</td>\n",
              "      <td>-0.139611</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>-0.062292</td>\n",
              "      <td>0.167098</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>-0.027951</td>\n",
              "      <td>-0.011140</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>-0.003128</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>0.001563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bem</th>\n",
              "      <td>-0.020953</td>\n",
              "      <td>0.009936</td>\n",
              "      <td>0.010063</td>\n",
              "      <td>0.272938</td>\n",
              "      <td>-0.065397</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>-0.008330</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>0.003251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>boi</th>\n",
              "      <td>-0.020953</td>\n",
              "      <td>0.009936</td>\n",
              "      <td>0.010063</td>\n",
              "      <td>0.272938</td>\n",
              "      <td>-0.065397</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>-0.008330</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>0.003251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bom</th>\n",
              "      <td>-0.459191</td>\n",
              "      <td>-0.258407</td>\n",
              "      <td>-0.143620</td>\n",
              "      <td>-0.088063</td>\n",
              "      <td>-0.107111</td>\n",
              "      <td>-0.198330</td>\n",
              "      <td>0.047333</td>\n",
              "      <td>-0.038604</td>\n",
              "      <td>-0.030473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                D0        D1        D2        D3        D4        D5  \\\n",
              "abencoe  -0.121141 -0.078354 -0.107089 -0.015278  0.005196  0.044912   \n",
              "achar    -0.020953  0.009936  0.010063  0.272938 -0.065397  0.009375   \n",
              "acoes    -0.014540 -0.011745 -0.010624 -0.015537 -0.137360 -0.330978   \n",
              "admirar  -0.014540 -0.011745 -0.010624 -0.015537 -0.137360 -0.330978   \n",
              "agora    -0.081228 -0.011599  0.188272 -0.026692  0.014662 -0.002089   \n",
              "amor     -0.006683  0.004699 -0.010784  0.049210  0.323950 -0.139611   \n",
              "apenas   -0.062292  0.167098 -0.037964 -0.027951 -0.011140  0.001665   \n",
              "aprender -0.006683  0.004699 -0.010784  0.049210  0.323950 -0.139611   \n",
              "atar     -0.062292  0.167098 -0.037964 -0.027951 -0.011140  0.001665   \n",
              "baixar   -0.081228 -0.011599  0.188272 -0.026692  0.014662 -0.002089   \n",
              "banana   -0.006683  0.004699 -0.010784  0.049210  0.323950 -0.139611   \n",
              "basto    -0.062292  0.167098 -0.037964 -0.027951 -0.011140  0.001665   \n",
              "bem      -0.020953  0.009936  0.010063  0.272938 -0.065397  0.009375   \n",
              "boi      -0.020953  0.009936  0.010063  0.272938 -0.065397  0.009375   \n",
              "bom      -0.459191 -0.258407 -0.143620 -0.088063 -0.107111 -0.198330   \n",
              "\n",
              "                D6        D7        D8  \n",
              "abencoe   0.016150 -0.011787 -0.008395  \n",
              "achar    -0.008330  0.005523  0.003251  \n",
              "acoes    -0.022575  0.012702  0.006081  \n",
              "admirar  -0.022575  0.012702  0.006081  \n",
              "agora     0.021458 -0.015946 -0.011367  \n",
              "amor     -0.001515  0.000986  0.000596  \n",
              "apenas   -0.003128  0.002280  0.001563  \n",
              "aprender -0.001515  0.000986  0.000596  \n",
              "atar     -0.003128  0.002280  0.001563  \n",
              "baixar    0.021458 -0.015946 -0.011367  \n",
              "banana   -0.001515  0.000986  0.000596  \n",
              "basto    -0.003128  0.002280  0.001563  \n",
              "bem      -0.008330  0.005523  0.003251  \n",
              "boi      -0.008330  0.005523  0.003251  \n",
              "bom       0.047333 -0.038604 -0.030473  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nEnQFBKQJF7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "42f37e60-8cb9-457c-db06-4950dc2492f2"
      },
      "cell_type": "code",
      "source": [
        "print('Matriz VT:')\n",
        "weights_vm = pd.DataFrame(v.T, columns=ldocs)\n",
        "display(weights_vm.head(15))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz VT:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.504540</td>\n",
              "      <td>-0.062465</td>\n",
              "      <td>0.852810</td>\n",
              "      <td>-0.095001</td>\n",
              "      <td>0.040895</td>\n",
              "      <td>-0.005758</td>\n",
              "      <td>0.047515</td>\n",
              "      <td>-0.031736</td>\n",
              "      <td>-0.016029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.386923</td>\n",
              "      <td>0.899867</td>\n",
              "      <td>-0.171965</td>\n",
              "      <td>-0.099485</td>\n",
              "      <td>-0.031072</td>\n",
              "      <td>0.004590</td>\n",
              "      <td>-0.006926</td>\n",
              "      <td>0.004539</td>\n",
              "      <td>0.002205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.752453</td>\n",
              "      <td>-0.421957</td>\n",
              "      <td>-0.485079</td>\n",
              "      <td>-0.054378</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.123808</td>\n",
              "      <td>0.035762</td>\n",
              "      <td>-0.023458</td>\n",
              "      <td>-0.011839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.041513</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>-0.048847</td>\n",
              "      <td>0.175148</td>\n",
              "      <td>0.903571</td>\n",
              "      <td>-0.384861</td>\n",
              "      <td>-0.003355</td>\n",
              "      <td>0.001963</td>\n",
              "      <td>0.000840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.001053</td>\n",
              "      <td>-0.000777</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>-0.002037</td>\n",
              "      <td>0.004359</td>\n",
              "      <td>0.010365</td>\n",
              "      <td>-0.625093</td>\n",
              "      <td>-0.664798</td>\n",
              "      <td>-0.408862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.037402</td>\n",
              "      <td>-0.020154</td>\n",
              "      <td>0.023620</td>\n",
              "      <td>-0.019270</td>\n",
              "      <td>0.019013</td>\n",
              "      <td>0.043164</td>\n",
              "      <td>-0.532904</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>-0.395007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.036408</td>\n",
              "      <td>-0.019436</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>-0.017655</td>\n",
              "      <td>0.016478</td>\n",
              "      <td>0.037306</td>\n",
              "      <td>-0.564657</td>\n",
              "      <td>0.026027</td>\n",
              "      <td>0.822377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.130147</td>\n",
              "      <td>0.053509</td>\n",
              "      <td>0.045582</td>\n",
              "      <td>0.971442</td>\n",
              "      <td>-0.182406</td>\n",
              "      <td>0.025844</td>\n",
              "      <td>-0.018445</td>\n",
              "      <td>0.010991</td>\n",
              "      <td>0.004585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.090313</td>\n",
              "      <td>-0.063251</td>\n",
              "      <td>-0.048125</td>\n",
              "      <td>-0.055299</td>\n",
              "      <td>-0.383129</td>\n",
              "      <td>-0.912398</td>\n",
              "      <td>-0.049989</td>\n",
              "      <td>0.025280</td>\n",
              "      <td>0.008575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         D0        D1        D2        D3        D4        D5        D6  \\\n",
              "0 -0.504540 -0.062465  0.852810 -0.095001  0.040895 -0.005758  0.047515   \n",
              "1 -0.386923  0.899867 -0.171965 -0.099485 -0.031072  0.004590 -0.006926   \n",
              "2 -0.752453 -0.421957 -0.485079 -0.054378  0.014493  0.123808  0.035762   \n",
              "3 -0.041513  0.025304 -0.048847  0.175148  0.903571 -0.384861 -0.003355   \n",
              "4 -0.001053 -0.000777  0.001357 -0.002037  0.004359  0.010365 -0.625093   \n",
              "5 -0.037402 -0.020154  0.023620 -0.019270  0.019013  0.043164 -0.532904   \n",
              "6 -0.036408 -0.019436  0.022418 -0.017655  0.016478  0.037306 -0.564657   \n",
              "7 -0.130147  0.053509  0.045582  0.971442 -0.182406  0.025844 -0.018445   \n",
              "8 -0.090313 -0.063251 -0.048125 -0.055299 -0.383129 -0.912398 -0.049989   \n",
              "\n",
              "         D7        D8  \n",
              "0 -0.031736 -0.016029  \n",
              "1  0.004539  0.002205  \n",
              "2 -0.023458 -0.011839  \n",
              "3  0.001963  0.000840  \n",
              "4 -0.664798 -0.408862  \n",
              "5  0.745000 -0.395007  \n",
              "6  0.026027  0.822377  \n",
              "7  0.010991  0.004585  \n",
              "8  0.025280  0.008575  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x7fW4lGxMygC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "99821c3c-2d17-4e4e-8498-5fe2625bb6c8"
      },
      "cell_type": "code",
      "source": [
        "SIMPLE = USE\n",
        "\n",
        "ws = np.zeros((len(ldocs), len(emotion_words.keys())))\n",
        "\n",
        "idx = { w:i for i, w in enumerate(weights.index.get_values())}\n",
        "\n",
        "for i, doc in enumerate(ldocs):\n",
        "    for k, item in enumerate(emotion_words.items()):\n",
        "        key, values = item\n",
        "        for value in values:\n",
        "            try:\n",
        "                if SIMPLE:\n",
        "                    index = weights.index.get_loc(value)\n",
        "                    idx_val = u[index]\n",
        "                    ws[i][k] += idx_val[i] * weights.iloc[index].values[i]\n",
        "                else:\n",
        "                    weight_sum = []\n",
        "                    indexes = filter(None, [e if value in inx else None for e, inx in enumerate(idx.keys())])\n",
        "                    for index in indexes:\n",
        "                        idx_val = u[index]\n",
        "                        weight_sum.append(idx_val[i] * weights.iloc[index].values[i])\n",
        "                    ws[i][k] += np.sum(weight_sum)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "ws = ws/len(ldocs)\n",
        "            \n",
        "df = pd.DataFrame(ws, columns=emotion_words.keys())\n",
        "display(df[\n",
        "    (df['ALEGRIA'] != 0) | (df['DESGOSTO'] != 0) | (df['MEDO'] != 0) | \n",
        "    (df['RAIVA'] != 0) | (df['SURPRESA'] != 0) | (df['TRISTEZA'] != 0)\n",
        "])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALEGRIA</th>\n",
              "      <th>DESGOSTO</th>\n",
              "      <th>MEDO</th>\n",
              "      <th>RAIVA</th>\n",
              "      <th>SURPRESA</th>\n",
              "      <th>TRISTEZA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.051021</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.018051</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.009025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.047873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.005468</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.028333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.001359</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALEGRIA  DESGOSTO  MEDO     RAIVA  SURPRESA  TRISTEZA\n",
              "0 -0.051021  0.000000   0.0 -0.018051  0.000000 -0.009025\n",
              "2 -0.047873  0.000000   0.0  0.000000  0.000000  0.000000\n",
              "3  0.005468  0.000000   0.0  0.000000  0.000000  0.000000\n",
              "4  0.000174  0.000000   0.0  0.000000  0.000000  0.000000\n",
              "5  0.000000  0.000000   0.0  0.000000  0.000000  0.001740\n",
              "6 -0.028333  0.000000   0.0  0.000000  0.000000  0.000000\n",
              "7  0.000000  0.000614   0.0  0.000000  0.000000  0.000000\n",
              "8 -0.001359  0.000000   0.0  0.000000  0.000676  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sDlsNlGnM_hM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "846240e0-c9dc-41b1-e8e7-c9df557643cb"
      },
      "cell_type": "code",
      "source": [
        "dtframe = np.zeros((len(ldocs), len(emotion_words.keys())))\n",
        "for i, d in enumerate(ldocs):\n",
        "    for k, it in enumerate(emotion_words.items()):\n",
        "        a = [ws[:, k]]\n",
        "        b = [v.T[i]]\n",
        "        dtframe[i][k] = cosine_similarity(a, b)\n",
        "dtframe = normalization(dtframe, -100, 100)\n",
        "       \n",
        "for i, frase in enumerate(frases):\n",
        "    print(f' D{i} - {frase}')\n",
        "\n",
        "df = pd.DataFrame(dtframe.T, index=emotion_words.keys(), columns=ldocs)\n",
        "display(df.head(15))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " D0 - bom dia senador agora claro porque pedagio nao baixar judiciario nao manifestar ser provocar cpis so servir pra corrupcao dar dar\n",
            " D1 - nao basto apenas retirar candidatar preferencial maioria eleitor brasileiro ter impedir tambem eleitor poder comparecer urna democracia gente poder judiciario comprometer atar cabelo golpe destroi pai\n",
            " D2 - deus abencoe dia todo voce ter um bom trabalhar bom estudar todo pra nao trabalhar estudar bom curticao suar cama\n",
            " D3 - aprender ter amor proprio banana q fazer umar tatuagem d\n",
            " D4 - estar muito feliz hoje\n",
            " D5 - dia chuvoso deixar triste\n",
            " D6 - hoje dia excelente\n",
            " D7 - ter certo coisa nao comer achar bem nojento ficar mastigar lingua boi exemplo\n",
            " D8 - admirar conseguir realizar bom acoes desejar nado trocar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ALEGRIA</th>\n",
              "      <td>-40.659673</td>\n",
              "      <td>38.471683</td>\n",
              "      <td>96.575440</td>\n",
              "      <td>-0.531021</td>\n",
              "      <td>21.819354</td>\n",
              "      <td>18.383686</td>\n",
              "      <td>17.069943</td>\n",
              "      <td>7.622796</td>\n",
              "      <td>3.524366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DESGOSTO</th>\n",
              "      <td>-14.863850</td>\n",
              "      <td>-9.985532</td>\n",
              "      <td>-13.750560</td>\n",
              "      <td>-10.331910</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>89.594016</td>\n",
              "      <td>-7.095701</td>\n",
              "      <td>-9.117806</td>\n",
              "      <td>-7.196166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEDO</th>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "      <td>-10.595886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAIVA</th>\n",
              "      <td>57.256302</td>\n",
              "      <td>41.438729</td>\n",
              "      <td>90.596394</td>\n",
              "      <td>-5.013095</td>\n",
              "      <td>-10.454300</td>\n",
              "      <td>-5.565992</td>\n",
              "      <td>-5.699620</td>\n",
              "      <td>6.906721</td>\n",
              "      <td>1.549705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SURPRESA</th>\n",
              "      <td>-12.751552</td>\n",
              "      <td>-10.299394</td>\n",
              "      <td>-12.188055</td>\n",
              "      <td>-10.482923</td>\n",
              "      <td>-65.580930</td>\n",
              "      <td>-63.717616</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>-9.979277</td>\n",
              "      <td>-9.442712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TRISTEZA</th>\n",
              "      <td>55.883171</td>\n",
              "      <td>40.614948</td>\n",
              "      <td>91.918691</td>\n",
              "      <td>-14.910678</td>\n",
              "      <td>-10.193016</td>\n",
              "      <td>-4.558175</td>\n",
              "      <td>-4.838498</td>\n",
              "      <td>7.248193</td>\n",
              "      <td>-21.895011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 D0         D1         D2         D3          D4         D5  \\\n",
              "ALEGRIA  -40.659673  38.471683  96.575440  -0.531021   21.819354  18.383686   \n",
              "DESGOSTO -14.863850  -9.985532 -13.750560 -10.331910 -100.000000  89.594016   \n",
              "MEDO     -10.595886 -10.595886 -10.595886 -10.595886  -10.595886 -10.595886   \n",
              "RAIVA     57.256302  41.438729  90.596394  -5.013095  -10.454300  -5.565992   \n",
              "SURPRESA -12.751552 -10.299394 -12.188055 -10.482923  -65.580930 -63.717616   \n",
              "TRISTEZA  55.883171  40.614948  91.918691 -14.910678  -10.193016  -4.558175   \n",
              "\n",
              "                  D6         D7         D8  \n",
              "ALEGRIA    17.069943   7.622796   3.524366  \n",
              "DESGOSTO   -7.095701  -9.117806  -7.196166  \n",
              "MEDO      -10.595886 -10.595886 -10.595886  \n",
              "RAIVA      -5.699620   6.906721   1.549705  \n",
              "SURPRESA  100.000000  -9.979277  -9.442712  \n",
              "TRISTEZA   -4.838498   7.248193 -21.895011  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "w2OjtRPnUrLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}