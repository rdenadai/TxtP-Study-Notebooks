{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/TxtP-Study-Notebooks/blob/master/notebooks/text_classification_example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WGxO__uG47K5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Análise e Validação de Textos em Português\n"
      ]
    },
    {
      "metadata": {
        "id": "AzVNkORP5dvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Referências:\n",
        "\n",
        " - [NLTK](http://www.nltk.org/howto/portuguese_en.html)\n",
        " - [spaCy](https://spacy.io/usage/spacy-101)\n",
        " - [Utilizando processamento de linguagem natural para criar um sumarização automática de textos](https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e)\n",
        " - [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
        " - [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)"
      ]
    },
    {
      "metadata": {
        "id": "fJl2_xgi5M4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Instalação"
      ]
    },
    {
      "metadata": {
        "id": "6gSt861Txvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1038
        },
        "outputId": "2dfc5b1f-5a94-403e-e3a6-4a59fa7a963b"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt\n",
        "!pip install feedparser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 45.7MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz (38.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.7MB 30.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "  Running setup.py install for pt-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed pt-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n",
            "Collecting feedparser\n",
            "  Using cached https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2\n",
            "Building wheels for collected packages: feedparser\n",
            "  Running setup.py bdist_wheel for feedparser ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built feedparser\n",
            "Installing collected packages: feedparser\n",
            "Successfully installed feedparser-5.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PcT7xbKKo55-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "7801495a-bed0-4446-d1fc-e12f1233067b"
      },
      "cell_type": "code",
      "source": [
        "# Download Oplexicon\n",
        "!rm -rf wget-log*\n",
        "!rm -rf oplexicon_v3.0\n",
        "!wget -O oplexicon_v3.0.zip https://github.com/rdenadai/sentiment-analysis-2018-president-election/blob/master/dataset/oplexicon_v3.0.zip?raw=true\n",
        "!unzip oplexicon_v3.0.zip\n",
        "!ls -lh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "Archive:  oplexicon_v3.0.zip\n",
            "  inflating: oplexicon_v3.0/lexico_v3.0.txt  \n",
            "  inflating: oplexicon_v3.0/README   \n",
            "total 120K\n",
            "drwxr-xr-x 2 root root 4.0K Sep 26 20:14 oplexicon_v3.0\n",
            "-rw-r--r-- 1 root root 102K Sep 26 20:14 oplexicon_v3.0.zip\n",
            "drwxr-xr-x 2 root root 4.0K Sep 25 01:36 sample_data\n",
            "-rw-r--r-- 1 root root 1.6K Sep 26 20:14 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVxM4zTo5QSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HLlEa6uEyX11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "45193832-d19d-4e47-c0e8-ebd6297de7fb"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('floresta')\n",
        "nltk.download('mac_morpho')\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import concurrent.futures\n",
        "import codecs\n",
        "import re\n",
        "import pprint\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import floresta as flt\n",
        "from nltk.corpus import machado as mch\n",
        "from nltk.corpus import mac_morpho as mcm\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Package floresta is already up-to-date!\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Package mac_morpho is already up-to-date!\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0CGK0ftshPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ]
    },
    {
      "metadata": {
        "id": "pOMbwFd3sd31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_oplexicon_data(filename):\n",
        "    spacy_conv = {\n",
        "        'adj': 'ADJ',\n",
        "        'n': 'NOUN',\n",
        "        'vb': 'VERB',\n",
        "        'det': 'DET',\n",
        "        'emot': 'EMOT',\n",
        "        'htag': 'HTAG'\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    with codecs.open(filename, 'r', 'UTF-8') as hf:\n",
        "        lines = hf.readlines()\n",
        "        for line in lines:\n",
        "            info = line.lower().split(',')\n",
        "            if len(info[0].split()) <= 1:\n",
        "                info[1] = [spacy_conv.get(tag) for tag in info[1].split()]\n",
        "                word, tags, sent = info[:3]\n",
        "                if 'HTAG' not in tags and 'EMOT' not in tags:\n",
        "                    word = word.replace('-se', '')\n",
        "                    stem = stemmer.stem(word)\n",
        "                    if stem in data:\n",
        "                        data[stem] += [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "                    else:\n",
        "                        data[stem] = [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwSQhPMk5Scp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ]
    },
    {
      "metadata": {
        "id": "mQzbtaNhy5Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "ec51eb0c-bf21-4fb1-b32a-7eb521653c36"
      },
      "cell_type": "code",
      "source": [
        "frase = u\"Gostaria de saber mais informações sobre a Amazon. Uma excelente loja de produtos online!\".lower()\n",
        "doc = nlp(frase)\n",
        "pp.pprint([(w.text, w.pos_) for w in doc])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   ('gostaria', 'VERB'),\n",
            "    ('de', 'ADP'),\n",
            "    ('saber', 'VERB'),\n",
            "    ('mais', 'DET'),\n",
            "    ('informações', 'NOUN'),\n",
            "    ('sobre', 'ADP'),\n",
            "    ('a', 'DET'),\n",
            "    ('amazon', 'NOUN'),\n",
            "    ('.', 'PUNCT'),\n",
            "    ('uma', 'DET'),\n",
            "    ('excelente', 'ADJ'),\n",
            "    ('loja', 'NOUN'),\n",
            "    ('de', 'ADP'),\n",
            "    ('produtos', 'NOUN'),\n",
            "    ('online', 'ADJ'),\n",
            "    ('!', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F--aytOy3gL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "bbc5096f-a707-429b-d353-83c3e794eff9"
      },
      "cell_type": "code",
      "source": [
        "opx = load_oplexicon_data('oplexicon_v3.0/lexico_v3.0.txt')\n",
        "print('Oplexicon size: ', len(opx))\n",
        "print('Examples: ')\n",
        "\n",
        "view = opx.items()\n",
        "pp.pprint(list(view)[:7])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oplexicon size:  10687\n",
            "Examples: \n",
            "[   ('ab-rog', [{'sentiment': '-1', 'tags': ['VERB'], 'word': ['ab-rogar']}]),\n",
            "    ('ababad', [{'sentiment': '0', 'tags': ['VERB'], 'word': ['ababadar']}]),\n",
            "    (   'ababel',\n",
            "        [   {'sentiment': '-1', 'tags': ['VERB'], 'word': ['ababelar']},\n",
            "            {'sentiment': '1', 'tags': ['VERB'], 'word': ['ababelar']}]),\n",
            "    ('abaçan', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abaçanar']}]),\n",
            "    ('abacin', [{'sentiment': '1', 'tags': ['VERB'], 'word': ['abacinar']}]),\n",
            "    (   'abaf',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafados']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafante']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abafantes']},\n",
            "            {'sentiment': '-1', 'tags': ['VERB'], 'word': ['abafar']}]),\n",
            "    (   'abaix',\n",
            "        [   {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixada']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixadas']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixado']},\n",
            "            {'sentiment': '-1', 'tags': ['ADJ'], 'word': ['abaixados']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']},\n",
            "            {'sentiment': '0', 'tags': ['VERB'], 'word': ['abaixar']}])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LEUFJNpSrw9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9b005b0b-0a9c-4a1d-dc92-7be18bbcb65a"
      },
      "cell_type": "code",
      "source": [
        "# d = feedparser.parse('http://rss.uol.com.br/feed/tecnologia.xml')\n",
        "# for entry in d['entries']:\n",
        "#     pp.pprint(entry['link'])\n",
        "\n",
        "stpwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "def rm_stop_words(palavra):\n",
        "    if palavra not in stpwords and not palavra.isdigit():\n",
        "        return re.sub(r'[\\\"\\'!@#$%&*\\(\\)-_=+{}\\[\\]:;>.<,|\\\\`´]', '', palavra.lower())\n",
        "    return None\n",
        "\n",
        "# Machado + Mac_Morpho\n",
        "print('Iniciando corpus...')\n",
        "N = 100000\n",
        "palavras = flt.words()[:N] + mch.words()[:N] + mcm.words()[:N]\n",
        "print('Corpus criado...')\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "    print('Executando processo de remoção das stopwords...')\n",
        "    palavras = procs.map(rm_stop_words, palavras, chunksize=50)\n",
        "print('Filtro e finalização...')\n",
        "palavras = filter(None, palavras)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando corpus...\n",
            "Corpus criado...\n",
            "Executando processo de remoção das stopwords...\n",
            "Filtro e finalização...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7CXkPtuVzio8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6145d166-8d37-455c-f43a-2333c4075d1b"
      },
      "cell_type": "code",
      "source": [
        "print('Tf-Idf:')\n",
        "vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 3))\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(palavras)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_tfidf.get_shape()[1])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-Idf:\n",
            "   Actual number of tfidf features: 27064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xIe-8XVdjjmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "23370616-25d5-43a9-d3cf-470ffd98c042"
      },
      "cell_type": "code",
      "source": [
        "weights = np.asarray(X_tfidf.mean(axis=0)).ravel().tolist()\n",
        "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
        "weights_df = weights_df.sort_values(by='weight', ascending=True)\n",
        "display(weights_df.head(10))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3020</th>\n",
              "      <td>averio</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24177</th>\n",
              "      <td>steved averio</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24176</th>\n",
              "      <td>steved</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aapenicheóptica</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13135</th>\n",
              "      <td>hidrovias</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13134</th>\n",
              "      <td>hidrelétricas</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13131</th>\n",
              "      <td>hichamarazi</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13129</th>\n",
              "      <td>hezbiislami</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13128</th>\n",
              "      <td>heterogeneidade</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13127</th>\n",
              "      <td>hessen</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  term    weight\n",
              "3020            averio  0.000004\n",
              "24177    steved averio  0.000004\n",
              "24176           steved  0.000004\n",
              "0      aapenicheóptica  0.000007\n",
              "13135        hidrovias  0.000007\n",
              "13134    hidrelétricas  0.000007\n",
              "13131      hichamarazi  0.000007\n",
              "13129      hezbiislami  0.000007\n",
              "13128  heterogeneidade  0.000007\n",
              "13127           hessen  0.000007"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xyW_HKxkwURE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "55ec19d3-6250-497b-cb97-86fe5a258ba2"
      },
      "cell_type": "code",
      "source": [
        "print(\"LSA:\")\n",
        "\n",
        "# Project the tfidf vectors onto the first N principal components.\n",
        "# Though this is significantly fewer features than the original tfidf vector,\n",
        "# they are stronger features, and the accuracy is higher.\n",
        "svd = TruncatedSVD(100)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_lsa = lsa.fit_transform(X_tfidf)\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(\"   Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA:\n",
            "   Explained variance of the SVD step: 14%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHnVMukg0oWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}